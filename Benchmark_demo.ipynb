{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Demo\n",
    "This demo is for reproducing the benchmark in:\n",
    "<br>Zhou et al. <b>\"Effective and Efficient Neural Networks for Spike Inference from In Vivo Calcium Imaging\"</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "\n",
    "<b>Hardwares:</b>\n",
    "- CUDA-enabled GPU\n",
    "- 24Gb of system RAM (recommended)\n",
    "\n",
    "<b>Sofwares and packages:</b>\n",
    "- python == 3.6\n",
    "- torch  >= 1.7.1 (with CUDA and cuDNN toolkits)\n",
    "- numpy  >= 1.19.2\n",
    "- scipy  >= 1.5.2\n",
    "- tqdm   >= 4.59.0\n",
    "\n",
    "- MATLAB (2017b - 2020b)\n",
    "    - Please install MATLAB engine for Python.\n",
    "    - Then include the <i>spikes-master</i> and <i>brick-master</i> folders as working path in MATLAB. \n",
    "\n",
    "<b>Note: </b>\n",
    "- The above requirements are for producing large-scale simulations in benchmark. \n",
    "- Minor variation in benchmark results may exist due to different setups as listed above. \n",
    "- For inference purposes in actual applications, any regular PC with a <b>CPU</b> is sufficient (see <i>ENS2_demo.ipynb</i> for details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "Before running this benchmark demo, please download the database (Rupprecht et al. 2021) from the below repository:\n",
    "<br>https://github.com/HelmchenLabSoftware/Cascade/tree/master/Ground_truth\n",
    "\n",
    "<b>Please put the downloaded dataset 1-27 into the \"ground_truth\" folder on a same file level as this notebook.</b>\n",
    "<br>For example:\n",
    "<br>.../ Benchmark_demo.ipynb\n",
    "<br>.../ ground_truth/ DS01-OGB1-m-V1/ (files)\n",
    "<br>.../ ground_truth/ DS02-OGB1-2-m-V1/ (files)\n",
    "<br>(etc...)\n",
    "<br>.../ ground_truth/ DS21-jGECO1a-m-V1/ (files)\n",
    "<br>(etc...)\n",
    "<br>.../ ground_truth/ DS27-GCaMP6f-m-PV-vivo-V1/ (files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import scipy.io as scio\n",
    "import copy\n",
    "import glob as glob\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "\n",
    "HalfTensor = torch.cuda.HalfTensor\n",
    "FloatTensor = torch.cuda.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global opt\n",
    "opt = argparse.ArgumentParser()\n",
    "opt.sampling_rate = 60     # re-sampling rate of data\n",
    "opt.smoothing_std = 0.025  # size of smoothing window (unit:s)\n",
    "opt.smoothing = opt.smoothing_std * opt.sampling_rate\n",
    "\n",
    "opt.causal_kernel = False\n",
    "opt.gaussian_kernel = not opt.causal_kernel # use Gaussian smoothing kernel\n",
    "    \n",
    "opt.signal_len = 96\n",
    "opt.classes = 6\n",
    "\n",
    "opt.lr = 0.001\n",
    "opt.epochs = 5000\n",
    "opt.patience = 500\n",
    "opt.batch_size = 1024\n",
    "opt.sample_interval = opt.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_ground_truth(sampling_rate):\n",
    "    ground_truth_folder='./ground_truth/'\n",
    "    datasets = glob.glob(os.path.join(ground_truth_folder, 'DS*'))\n",
    "    dataset_trial = dict()\n",
    "    dataset_neuron = dict()\n",
    "    dataset_prop = dict()\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        files = glob.glob(os.path.join(dataset, '*'))\n",
    "        recording_trial_list = list()\n",
    "        recording_neuron_list = list()\n",
    "        recording_sample_count = 0\n",
    "        recording_raw_fs = []\n",
    "        recording_noise_level = []\n",
    "        recording_firing_rate = []\n",
    "        for i, file in enumerate(files):\n",
    "            try:\n",
    "                recording_neuron, recording_trial, count = load_recordings_from_file(file, sampling_rate)\n",
    "                recording_neuron_list.extend(recording_neuron)\n",
    "                recording_trial_list.extend(recording_trial)\n",
    "                recording_sample_count += count\n",
    "                recording_raw_fs.append(recording_neuron[0]['frame_rate'])\n",
    "                recording_noise_level.append(recording_neuron[0]['noise_level'])\n",
    "                recording_firing_rate.append(recording_neuron[0]['firing_rate'])\n",
    "            except:\n",
    "                print('Problem loading file {} from {}'.format(i+1, dataset))\n",
    "                pass\n",
    "        dataset_neuron[j+1] = recording_neuron_list\n",
    "        dataset_trial[j+1] = recording_trial_list\n",
    "        dataset_prop[j+1] = dict()\n",
    "        dataset_prop[j+1]['name'] = os.path.basename(dataset)\n",
    "        dataset_prop[j+1]['dset'] = j+1\n",
    "        dataset_prop[j+1]['frame_rate'] = np.mean(recording_raw_fs)\n",
    "        dataset_prop[j+1]['noise_level'] = np.mean(recording_noise_level)\n",
    "        dataset_prop[j+1]['firing_rate'] = np.mean(recording_firing_rate)\n",
    "        dataset_prop[j+1]['neuron_number'] = len(recording_neuron_list)\n",
    "        dataset_prop[j+1]['trial_number'] = len(recording_trial_list)\n",
    "        dataset_prop[j+1]['duration'] = int(recording_sample_count/sampling_rate/60)\n",
    "\n",
    "        print(f'[D{j+1:2d}]  Raw Fs: {np.mean(recording_raw_fs):>5.1f}  Noise: {np.mean(recording_noise_level):>4.1f}  FR: {np.mean(recording_firing_rate):>4.1f}  #Neuron: {len(recording_neuron_list):3d}  #Trial: {len(recording_trial_list):3d}  #Sample: {recording_sample_count:7d}  Duration: {int(recording_sample_count/sampling_rate/60):4d}  Name: {os.path.basename(dataset)[:20]}')\n",
    "    return dataset_neuron, dataset_trial, dataset_prop\n",
    "\n",
    "def build_causal_kernel(sampling_rate):\n",
    "    xx = np.arange(0,199)/sampling_rate\n",
    "    yy = scipy.stats.invgauss.pdf(xx,opt.smoothing/sampling_rate*2.0,101/sampling_rate,1)\n",
    "    ix = np.argmax(yy)\n",
    "    yy = np.roll(yy,int((99-ix)/1.5))\n",
    "    causal_smoothing_kernel = yy/np.nansum(yy)\n",
    "    return causal_smoothing_kernel\n",
    "        \n",
    "def load_recordings_from_file(file_path, sampling_rate):\n",
    "\n",
    "    data = scio.loadmat(file_path)['CAttached'][0]\n",
    "\n",
    "    recording_trial = list()\n",
    "\n",
    "    trace_seq, spike_seq, rate_seq = np.zeros((0, 1),dtype='float32'), np.zeros((0,),dtype='int'), np.zeros((0,),dtype='float32')\n",
    "    trace_seg, spike_seg, rate_seg = np.zeros((0, opt.signal_len),dtype='float32'), np.zeros((0, opt.signal_len),dtype='int'), np.zeros((0, opt.signal_len),dtype='float32')\n",
    "    spike_num, rate_num, class_num = np.zeros((0, 1),dtype='int'), np.zeros((0, 1),dtype='float32'), np.zeros((0, 1),dtype='int')\n",
    "    \n",
    "    # record sampling rate after processing\n",
    "    FS, FS_resampled = [],[]\n",
    "    \n",
    "    # for calculation of neuron-wise noise level\n",
    "    concat_traces_mean = np.zeros((0,),dtype='float32')\n",
    "    \n",
    "    # for calculation of neuron-wise firing rate\n",
    "    concat_events = []\n",
    "    concat_times = []\n",
    "    \n",
    "    # for calibration of ER computation\n",
    "    concat_events_times = []\n",
    "    cum_times = 0\n",
    "    \n",
    "    for i,trial in enumerate(data):\n",
    "        # find the relevant elements in the data structure\n",
    "        # (dF/F traces; spike events; time stamps of fluorescence recording)\n",
    "        keys = trial[0][0].dtype.descr\n",
    "        keys_unfolded = list(sum(keys, ()))\n",
    "\n",
    "        try:\n",
    "            traces_index = int(keys_unfolded.index(\"fluo_mean\")/2)\n",
    "            fluo_time_index = int(keys_unfolded.index(\"fluo_time\")/2)\n",
    "            events_index = int(keys_unfolded.index(\"events_AP\")/2)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # spikes\n",
    "        events = trial[0][0][events_index]\n",
    "        events = events[~np.isnan(events)]\n",
    "        ephys_sampling_rate = 1e4\n",
    "        event_time = events/ephys_sampling_rate\n",
    "        \n",
    "        # fluorescence\n",
    "        fluo_times = np.squeeze(trial[0][0][fluo_time_index])\n",
    "        traces_mean = np.squeeze(trial[0][0][traces_index])\n",
    "        traces_mean = traces_mean[:fluo_times.shape[0]]\n",
    "\n",
    "        traces_mean = traces_mean[~np.isnan(fluo_times)]\n",
    "        fluo_times = fluo_times[~np.isnan(fluo_times)]\n",
    "\n",
    "        frame_rate = 1/np.mean(np.diff(fluo_times))\n",
    "\n",
    "        # concatenate for statistics\n",
    "        concat_traces_mean = np.concatenate([concat_traces_mean,traces_mean], axis=0)\n",
    "        concat_events.append(len(events))\n",
    "        concat_times.append(fluo_times[-1])\n",
    "        \n",
    "        # calibrate onset time\n",
    "        event_time = event_time[np.logical_and(fluo_times[0]<=event_time,event_time<=fluo_times[-1])]\n",
    "        event_time = event_time - fluo_times[0] + 1/frame_rate\n",
    "        fluo_times = fluo_times - fluo_times[0] + 1/frame_rate\n",
    "        if event_time.size==0:\n",
    "            continue\n",
    "\n",
    "        # resampling\n",
    "        num_samples = int(round(traces_mean.shape[0]*sampling_rate/frame_rate))\n",
    "        (traces_mean,fluo_times_resampled) = scipy.signal.resample(traces_mean,num_samples,np.squeeze(fluo_times),axis=0)\n",
    "        frame_rate_resampled = 1/np.nanmean(np.diff(fluo_times_resampled))\n",
    "        \n",
    "        # calibrate bin size\n",
    "        fluo_times_resampled = fluo_times_resampled*frame_rate_resampled/sampling_rate\n",
    "        frame_rate_resampled = 1/np.nanmean(np.diff(fluo_times_resampled))\n",
    "        \n",
    "        # cleaning data\n",
    "        num_samples -= int(np.floor(frame_rate_resampled))\n",
    "        traces_mean = traces_mean[int(np.ceil(0.5*frame_rate_resampled)):int(np.ceil(0.5*frame_rate_resampled))+num_samples]\n",
    "        fluo_times_resampled = fluo_times_resampled[int(np.ceil(0.5*frame_rate_resampled)):int(np.ceil(0.5*frame_rate_resampled))+num_samples]\n",
    "        \n",
    "        # calibration again\n",
    "        if event_time.size==0 or fluo_times_resampled.size==0:\n",
    "            continue\n",
    "        event_time = event_time[np.logical_and(fluo_times_resampled[0]<=event_time,event_time<=fluo_times_resampled[-1])]\n",
    "        event_time = event_time - fluo_times_resampled[0] + 1/frame_rate_resampled\n",
    "        fluo_times_resampled = fluo_times_resampled - fluo_times_resampled[0] + 1/frame_rate_resampled\n",
    "        if event_time.size==0:\n",
    "            continue\n",
    "\n",
    "        # bin the ground truth (spike times) into time bins determined by the resampled calcium recording\n",
    "        fluo_times_bin_centers = fluo_times_resampled\n",
    "        fluo_times_bin_edges = np.append(fluo_times_bin_centers,fluo_times_bin_centers[-1]+1/frame_rate_resampled/2) - 1/frame_rate_resampled/2\n",
    "\n",
    "        [events_binned,event_bins] = np.histogram(event_time, bins=fluo_times_bin_edges)\n",
    "\n",
    "        # concatenate event_time\n",
    "        concat_events_times = np.concatenate([concat_events_times, event_time+cum_times])\n",
    "        cur_times = len(traces_mean)/frame_rate_resampled\n",
    "        cum_times += cur_times\n",
    "\n",
    "        # trial-wise firing rate\n",
    "        firing_rate = sum(events_binned)/(len(event_bins)/frame_rate_resampled)\n",
    "        \n",
    "        # trial-wise noise level\n",
    "        noise_level = np.nanmedian(np.abs(np.diff(traces_mean)))*100/np.sqrt(frame_rate)\n",
    "        \n",
    "        # do pre-processing here if needed\n",
    "        traces_mean = np.expand_dims(traces_mean, 0)\n",
    "        \n",
    "        # padding incase recording is too short\n",
    "        traces_mean = np.concatenate([np.zeros((1,opt.signal_len//2)), traces_mean, np.zeros((1,opt.signal_len//2))], axis=1)\n",
    "        events_binned = np.concatenate([np.zeros((opt.signal_len//2,)), events_binned, np.zeros((opt.signal_len//2,))], axis=0)\n",
    "        \n",
    "        # smooth spikes to facilitate gradient descents\n",
    "        if opt.causal_kernel:\n",
    "            if 'causal_smoothing_kernel' not in locals():\n",
    "                causal_smoothing_kernel = build_causal_kernel(sampling_rate)            \n",
    "            events_binned_smooth = np.convolve(events_binned.astype(float),causal_smoothing_kernel,mode='same')\n",
    "        if opt.gaussian_kernel:\n",
    "            events_binned_smooth = scipy.ndimage.filters.gaussian_filter(events_binned.astype(float), sigma=opt.smoothing_std*sampling_rate)\n",
    "        \n",
    "        # format data into segments\n",
    "        data_len = len(events_binned)-opt.signal_len\n",
    "        before = int(opt.signal_len//2)\n",
    "        after = int(opt.signal_len//2-1)\n",
    "        \n",
    "        X = np.zeros((data_len, opt.signal_len), dtype='float32')\n",
    "        YY_spike = np.zeros((data_len, opt.signal_len), dtype='int')\n",
    "        YY_rate = np.zeros((data_len, opt.signal_len), dtype='float32')\n",
    "        Y_spike = np.zeros((data_len, 1), dtype='int')\n",
    "        Y_rate = np.zeros((data_len, 1), dtype='float32')\n",
    "        Y_class = np.zeros((data_len, 1), dtype='int')\n",
    "\n",
    "        traces_mean = traces_mean.astype('float32')\n",
    "        events_binned = events_binned.astype('int')\n",
    "        events_binned_smooth = events_binned_smooth.astype('float32')\n",
    "        \n",
    "        for time_point in range(data_len):\n",
    "            X[time_point,:] = traces_mean[:,time_point:time_point+opt.signal_len]\n",
    "            YY_spike[time_point,:] = events_binned[time_point:time_point+opt.signal_len]\n",
    "            YY_rate[time_point,:] = events_binned_smooth[time_point:time_point+opt.signal_len]\n",
    "            Y_spike[time_point] = events_binned[time_point+before]\n",
    "            Y_rate[time_point] = events_binned_smooth[time_point+before]\n",
    "            Y_class[time_point] = events_binned[time_point+before]\n",
    "        Y_class[Y_class>=opt.classes] = opt.classes-1\n",
    "        \n",
    "        recording_trial.append(dict(time_resampled=fluo_times_resampled, \n",
    "                                    frame_rate=frame_rate,frame_rate_resampled=frame_rate_resampled,\n",
    "                                    firing_rate=firing_rate,\n",
    "                                    noise_level=noise_level,\n",
    "                                    trace_seq=traces_mean[:,before:before+data_len].T, \n",
    "                                    spike_seq=events_binned[before:before+data_len], \n",
    "                                    rate_seq=events_binned_smooth[before:before+data_len],\n",
    "                                    trace_seg=X, spike_seg=YY_spike, rate_seg=YY_rate,\n",
    "                                    spike_num=Y_spike, rate_num=Y_rate, class_num=Y_class,\n",
    "                                    events_times=event_time, elapsed_times=cur_times))\n",
    "\n",
    "        FS.append(frame_rate)\n",
    "        FS_resampled.append(frame_rate_resampled)\n",
    "        \n",
    "        trace_seq = np.concatenate([trace_seq, traces_mean[:,before:before+data_len].T], axis=0)\n",
    "        spike_seq = np.concatenate([spike_seq, events_binned[before:before+data_len]], axis=0)\n",
    "        rate_seq = np.concatenate([rate_seq, events_binned_smooth[before:before+data_len]], axis=0)\n",
    "        trace_seg = np.concatenate([trace_seg, X], axis=0)\n",
    "        spike_seg = np.concatenate([spike_seg, YY_spike], axis=0)\n",
    "        rate_seg = np.concatenate([rate_seg, YY_rate], axis=0)\n",
    "        spike_num = np.concatenate([spike_num, Y_spike], axis=0)\n",
    "        rate_num = np.concatenate([rate_num, Y_rate], axis=0)\n",
    "        class_num = np.concatenate([class_num, Y_class], axis=0)\n",
    "        \n",
    "    recording_neuron = [dict(frame_rate=np.nanmean(FS), frame_rate_resampled=np.nanmean(FS_resampled), \n",
    "                             firing_rate=np.sum(concat_events)/np.sum(concat_times),\n",
    "                             noise_level=np.nanmedian(np.abs(np.diff(concat_traces_mean)))*100/np.sqrt(np.nanmean(FS)),\n",
    "                             trace_seq=trace_seq, spike_seq=spike_seq, rate_seq=rate_seq,\n",
    "                             trace_seg=trace_seg, spike_seg=spike_seg, rate_seg=rate_seg,\n",
    "                             spike_num=spike_num, rate_num=rate_num, class_num=class_num,\n",
    "                             events_times=concat_events_times, elapsed_times=cum_times)]\n",
    "    \n",
    "    return recording_neuron, recording_trial, trace_seg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# datasets, datasets_raw, datasets_prop = load_all_ground_truth(opt.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy algorithm to estimate spike-event from spike-rate\n",
    "\n",
    "def estimate_spike(rate, std=opt.smoothing):\n",
    "    rate = np.float32(np.array(copy.deepcopy(rate))).squeeze()\n",
    "    # remove bubbles produced by neural network\n",
    "    rate[rate<0.02/std] = 0\n",
    "    # initialize\n",
    "    rate_diff = np.diff(np.int8(rate>0))\n",
    "    est_spike = np.zeros(rate.shape, dtype='float32')\n",
    "    est_rate = np.zeros(rate.shape, dtype='float32')\n",
    "    onset, offset = 0, 0\n",
    "    for idx in range(len(rate_diff)):\n",
    "        # locate each piece of slices with spike rate\n",
    "        if rate_diff[idx] == 1:\n",
    "            onset = idx+1\n",
    "        elif rate_diff[idx] == -1:\n",
    "            if onset > 0:\n",
    "                offset = idx\n",
    "                # extract pieces of slices\n",
    "                slices = rate[onset:offset+1]\n",
    "                # at least one spike is included when probability is over 0.5\n",
    "                could_add = True\n",
    "                cur_spike = np.zeros(slices.shape, dtype='float32')\n",
    "                if np.sum(slices)>=0.5:\n",
    "                    cur_spike[np.argmax(slices)] = 1\n",
    "                cur_rate = scipy.ndimage.filters.gaussian_filter(cur_spike, sigma=std, mode='constant', cval=0.)\n",
    "                cur_loss = np.sum((slices-cur_rate)**2)\n",
    "                # iteratively insert spikes that are best-match\n",
    "                while could_add:\n",
    "                    candidate_spike = cur_spike + np.eye(len(slices),len(slices),dtype='float32')\n",
    "                    candidate_rate = scipy.ndimage.filters.gaussian_filter(candidate_spike, sigma=(0,std), mode='constant', cval=0.)\n",
    "                    candidate_loss = np.sum(np.power(slices-candidate_rate,2),1)\n",
    "                    new_loss, new_loss_idx = np.amin(candidate_loss), np.argmin(candidate_loss)\n",
    "                    if new_loss - cur_loss <= -0.00000001:\n",
    "                        cur_spike = candidate_spike[new_loss_idx,:]\n",
    "                        cur_rate = candidate_rate[new_loss_idx,:]\n",
    "                        cur_loss = new_loss\n",
    "                        could_add = True\n",
    "                    else:\n",
    "                        est_spike[onset:offset+1] = cur_spike\n",
    "                        est_rate[onset:offset+1] = cur_rate\n",
    "                        could_add = False\n",
    "        # force estimation with maximum slice length of 500 data points\n",
    "        elif idx - onset >= 500-1:\n",
    "            if len(rate_diff) > idx+1:\n",
    "                rate_diff[idx+1] = -1\n",
    "            if len(rate_diff) > idx+2:\n",
    "                rate_diff[idx+2] = 1\n",
    "    return est_spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract spike-event timestamps from spike trains\n",
    "\n",
    "def extract_event(spike):\n",
    "    spike_input = np.squeeze(copy.deepcopy(spike))\n",
    "    event_output = []\n",
    "    while np.sum(spike_input>0):\n",
    "        event_output += ((np.where(spike_input>0)[0]+1)/opt.sampling_rate).tolist()\n",
    "        spike_input -= 1\n",
    "    event_output.sort()\n",
    "    return event_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize models\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Corrleation based loss function\n",
    "\n",
    "def pearson_corr_loss(output, target):\n",
    "    vx = torch.squeeze(output.reshape(-1,1))\n",
    "    vy = torch.squeeze(target.reshape(-1,1))\n",
    "    vx = vx - torch.mean(vx)\n",
    "    vy = vy - torch.mean(vy)\n",
    "    batch_loss = torch.dot(vx, vy) / (torch.norm(vx) * torch.norm(vy) + 1e-7)\n",
    "    return batch_loss\n",
    "\n",
    "# Custom van Rossum distance based loss function\n",
    "\n",
    "def eucd_loss(output, target_rate, target_spike):\n",
    "    vx = torch.squeeze(output.reshape(-1,1))\n",
    "    vy = torch.squeeze(target_rate.reshape(-1,1))\n",
    "    vz = torch.squeeze(target_spike.reshape(-1,1))\n",
    "    eucd_loss = torch.sqrt(torch.sum((vx-vy)**2)/opt.smoothing_std/torch.sum(vz))\n",
    "    return eucd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement used in CASCADE (Correlation, Error, and Bias)\n",
    "\n",
    "def evaluate_cascade(predict_input, truth_input):\n",
    "    predict = np.squeeze(copy.deepcopy(predict_input))\n",
    "    predict[predict<0] = 0\n",
    "    truth = np.squeeze(copy.deepcopy(truth_input))\n",
    "    truth[truth<0] = 0\n",
    "    corr = np.corrcoef(predict, truth)[0,-1]\n",
    "    diff = predict - truth\n",
    "    FP = np.sum(np.abs(diff[diff>0]))\n",
    "    FN = np.sum(np.abs(diff[diff<0]))\n",
    "    denorm = np.sum(np.abs(truth))\n",
    "    error = (FP+FN)/denorm\n",
    "    bias = (FP-FN)/denorm\n",
    "    return corr, error, bias\n",
    "\n",
    "# van Rossum distance measurement\n",
    "\n",
    "def evaluate_eucd(predict, truth_rate, truth_spike):\n",
    "    test_eucd = np.sqrt(np.sum((predict.squeeze()-truth_rate.squeeze())**2)/opt.smoothing_std/np.sum(truth_spike))\n",
    "    return test_eucd\n",
    "\n",
    "# Error rate measurement\n",
    "\n",
    "def evaluate_er(predict_input, truth_input, win=0.05):\n",
    "    predict, truth = copy.deepcopy(predict_input), copy.deepcopy(truth_input)\n",
    "    tli, tlj = [], []\n",
    "    for i in predict:\n",
    "        tli.append(float(i))\n",
    "    for j in truth:\n",
    "        tlj.append(float(j))\n",
    "    tli.sort()\n",
    "    tlj.sort()\n",
    "    test_er = eng.computeER(eng.cell2mat(tli),eng.cell2mat(tlj),eng.double(win))\n",
    "    return test_er\n",
    "\n",
    "# Victor-Purpura distance measurement\n",
    "\n",
    "def evaluate_vpd(predict_input, truth_input, cost=1):\n",
    "    predict, truth = copy.deepcopy(predict_input), copy.deepcopy(truth_input)\n",
    "    tli, tlj = [], []\n",
    "    for i in predict:\n",
    "        tli.append(float(i))\n",
    "    for j in truth:\n",
    "        tlj.append(float(j))\n",
    "    tli.sort()\n",
    "    tlj.sort()\n",
    "    test_pvd = np.float(eng.spkd(eng.cell2mat(tli), eng.cell2mat(tlj), eng.double(cost))) / len(tlj)\n",
    "    return test_pvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Benchmark in a Leave-one-dataset-out Manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BENCHMARK(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.DATA = [[]]*27\n",
    "        self.MODEL = {}\n",
    "        \n",
    "        self.cluster = {2:[2,4],3:[3,5],4:[2,4],5:[3,5],\n",
    "                        6:[6,7,8,9,10,11],7:[6,7,8,9,10,11],8:[6,7,8,9,10,11],9:[6,7,8,9,10,11],10:[6,7,8,9,10,11],11:[6,7,8,9,10,11],\n",
    "                        12:[12,13,14,15,16],13:[12,13,14,15,16],14:[12,13,14,15,16],15:[12,13,14,15,16],16:[12,13,14,15,16],\n",
    "                        17:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                        18:[18,19],19:[18,19],\n",
    "                        20:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],21:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                        22:[22,23],23:[22,23],\n",
    "                        24:[24,25,26,27],25:[24,25,26,27],26:[24,25,26,27],27:[24,25,26,27]}\n",
    "        \n",
    "        self.anticluster = {2:[2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            3:[2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            4:[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            5:[2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            6:[2,3,4,5,6,12,13,14,15,16,17,18,19,20,21],\n",
    "                            7:[2,3,4,5,7,12,13,14,15,16,17,18,19,20,21],\n",
    "                            8:[2,3,4,5,8,12,13,14,15,16,17,18,19,20,21],\n",
    "                            9:[2,3,4,5,9,12,13,14,15,16,17,18,19,20,21],\n",
    "                            10:[2,3,4,5,10,12,13,14,15,16,17,18,19,20,21],\n",
    "                            11:[2,3,4,5,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            12:[2,3,4,5,6,7,8,9,10,11,12,17,18,19,20,21],\n",
    "                            13:[2,3,4,5,6,7,8,9,10,11,13,17,18,19,20,21],\n",
    "                            14:[2,3,4,5,6,7,8,9,10,11,14,17,18,19,20,21],\n",
    "                            15:[2,3,4,5,6,7,8,9,10,11,15,17,18,19,20,21],\n",
    "                            16:[2,3,4,5,6,7,8,9,10,11,16,17,18,19,20,21],\n",
    "                            17:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            18:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,21],\n",
    "                            19:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,19,20,21],\n",
    "                            20:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            21:[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
    "                            22:[22,24,25,26,27],\n",
    "                            23:[23,24,25,26,27],\n",
    "                            24:[22,23,24],25:[22,23,25],26:[22,23,26],27:[22,23,27]}\n",
    "        \n",
    "        \n",
    "    def train(self, neuron='Exc', inputs='Raw', nets='UNet', losses='MSE', Fs='60', smoothing_std='0.025', smoothing_kernel='gaussian',\n",
    "              cluster='None', hour='all', lr='0.001', kernel='3', node='150K', seg='96', batch='1024', es='500', verbose=0):\n",
    "        \n",
    "        global opt\n",
    "        #### define test mode\n",
    "        self.TEST = 'C'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d')[2:]+'_'+neuron+'_'+Fs+'Hz_'+inputs+'_'+nets+'_'+losses+'_Test'\n",
    "\n",
    "        if smoothing_std != '0.025':\n",
    "            self.TEST = self.TEST+'_'+str(int(float(smoothing_std)*1000))+'ms'\n",
    "        if smoothing_kernel == 'gaussian':\n",
    "            opt.causal_kernel = False\n",
    "            opt.gaussian_kernel = not opt.causal_kernel # use Gaussian smoothing kernel\n",
    "        elif smoothing_kernel == 'causal':\n",
    "            opt.causal_kernel = True\n",
    "            opt.gaussian_kernel = not opt.causal_kernel # use causal smoothing kernel\n",
    "            self.TEST = self.TEST+'_'+'causal'\n",
    "        else:\n",
    "            print(\"Smoothing kernel error...\")\n",
    "            return\n",
    "            \n",
    "        if cluster != 'None':\n",
    "            self.TEST = self.TEST+'_'+cluster\n",
    "        if hour != 'all':\n",
    "            self.TEST = self.TEST+'_'+hour+'hr'\n",
    "        \n",
    "        if kernel != '3':\n",
    "            self.TEST = self.TEST+'_'+kernel+'kernel'\n",
    "        if node != '150K':\n",
    "            self.TEST = self.TEST+'_'+node+'node'\n",
    "            \n",
    "        if lr != '0.001':\n",
    "            opt.lr = float(lr)\n",
    "            self.TEST = self.TEST+'_'+lr+'lr'\n",
    "        if seg != '96':\n",
    "            opt.signal_len = int(seg)\n",
    "            self.TEST = self.TEST+'_'+seg+'seg'\n",
    "        if batch != '1024':\n",
    "            opt.batch_size = int(batch)\n",
    "            self.TEST = self.TEST+'_'+batch+'batch'\n",
    "        if es != '500':\n",
    "            opt.patience = int(es)\n",
    "            self.TEST = self.TEST+'_'+es+'es'\n",
    "            \n",
    "        print('【'+self.TEST+'】')\n",
    "        fs_spec = (0,0)\n",
    "        datasets, datasets_prop = [], []\n",
    "        \n",
    "        if neuron == 'Both':\n",
    "            ds_on, ds_off = 2, 27\n",
    "        elif neuron == 'Exc':\n",
    "            ds_on, ds_off = 2, 21\n",
    "        elif neuron == 'Inh':\n",
    "            ds_on, ds_off = 22, 27\n",
    "        else:\n",
    "            print(\"Neuron type error...\")\n",
    "            return\n",
    "        \n",
    "        for dsets in range(ds_on, ds_off+1):\n",
    "                        \n",
    "            tqdm.write(f'dataset {dsets}: preparing data...')\n",
    "            \n",
    "            #### load or re-compile data\n",
    "            opt.sampling_rate = float(Fs)\n",
    "            opt.smoothing_std = float(smoothing_std)\n",
    "            opt.smoothing = opt.smoothing_std * opt.sampling_rate\n",
    "                    \n",
    "            tqdm.write(f'Sampling rate is: {opt.sampling_rate}Hz, smoothing window is: {opt.smoothing_std*1000}ms')\n",
    "            \n",
    "            if fs_spec == (opt.sampling_rate, opt.smoothing_std):\n",
    "                print('Using previous datasets...')\n",
    "            else:\n",
    "                print('Re-compiling datasets...')\n",
    "                del datasets, datasets_prop\n",
    "                datasets, _, datasets_prop = load_all_ground_truth(opt.sampling_rate)\n",
    "                fs_spec = (opt.sampling_rate, opt.smoothing_std)\n",
    "\n",
    "            #### initialize vault\n",
    "            self.DATA[dsets-1] = {}\n",
    "            self.DATA[dsets-1]['inputs'] = inputs\n",
    "            self.DATA[dsets-1]['nets'] = nets\n",
    "            self.DATA[dsets-1]['losses'] = losses\n",
    "            self.DATA[dsets-1]['dataset'] = dsets\n",
    "            self.DATA[dsets-1]['frame_rate'] = datasets_prop[dsets]['frame_rate']\n",
    "            self.DATA[dsets-1]['noise_level'] = datasets_prop[dsets]['noise_level']\n",
    "            self.DATA[dsets-1]['firing_rate'] = datasets_prop[dsets]['firing_rate']\n",
    "            self.DATA[dsets-1]['neuron_number'] = datasets_prop[dsets]['neuron_number']\n",
    "            \n",
    "            self.DATA[dsets-1]['sampling_rate'] = opt.sampling_rate\n",
    "            self.DATA[dsets-1]['correlation'] = []\n",
    "            self.DATA[dsets-1]['error'] = []\n",
    "            self.DATA[dsets-1]['bias'] = []\n",
    "            self.DATA[dsets-1]['eucd'] = []\n",
    "            self.DATA[dsets-1]['vpd'] = []\n",
    "            self.DATA[dsets-1]['er50'] = []\n",
    "            self.DATA[dsets-1]['er100'] = []\n",
    "            self.DATA[dsets-1]['er500'] = []\n",
    "            self.DATA[dsets-1]['gter50'] = []\n",
    "            self.DATA[dsets-1]['loss'] = []\n",
    "\n",
    "            self.DATA[dsets-1]['calcium'] = []\n",
    "            self.DATA[dsets-1]['gt_rate'] = []\n",
    "            self.DATA[dsets-1]['gt_spike'] = []\n",
    "            self.DATA[dsets-1]['pd_rate'] = []\n",
    "            self.DATA[dsets-1]['pd_spike'] = []\n",
    "            self.DATA[dsets-1]['gt_event'] = []\n",
    "            self.DATA[dsets-1]['pd_event'] = []\n",
    "            self.DATA[dsets-1]['events_times'] = []\n",
    "            \n",
    "            start_time = datetime.datetime.now()\n",
    "            self.model_ver = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d%H%M%S')[2:]+'_dsets'+str(dsets)+'_'+str(opt.sampling_rate)+'Hz_'+inputs+'_'+nets+'_'+losses\n",
    "\n",
    "            self.DATA[dsets-1]['model_ver'] = self.model_ver\n",
    "            \n",
    "            #### prepare data\n",
    "            random.seed(0)\n",
    "            torch.manual_seed(0)\n",
    "            np.random.seed(0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            cudnn.deterministic = True\n",
    "            cudnn.benchmark = False\n",
    "            \n",
    "            test_trace = []\n",
    "            test_rate = []\n",
    "            test_spike = []\n",
    "            test_event = []\n",
    "            train_trace = np.zeros((800*10000,opt.signal_len),dtype='float32')\n",
    "            train_rate = np.zeros((800*10000,opt.signal_len),dtype='float32')\n",
    "            train_spike = np.zeros((800*10000,opt.signal_len),dtype='float32')\n",
    "            train_trace[:] = np.nan\n",
    "            train_rate[:] = np.nan\n",
    "            train_spike[:] = np.nan\n",
    "            count = 0\n",
    "            cum_time = 0\n",
    "\n",
    "            if cluster == 'cluster':\n",
    "                dset_indexes = self.cluster[dsets]\n",
    "            elif cluster == 'anticluster':\n",
    "                dset_indexes = self.anticluster[dsets]\n",
    "            else:\n",
    "                dset_indexes = np.arange(ds_on, ds_off+1)\n",
    "                \n",
    "            for dset_index in dset_indexes:\n",
    "                if dset_index == dsets:\n",
    "                    for trial_index in range(len(datasets[dset_index])):\n",
    "                        test_trace.append(datasets[dset_index][trial_index]['trace_seg'])\n",
    "                        test_rate.append(datasets[dset_index][trial_index]['rate_seg'])\n",
    "                        test_spike.append(datasets[dset_index][trial_index]['spike_seg'])\n",
    "                        test_event.append(datasets[dset_index][trial_index]['events_times'])\n",
    "                else:\n",
    "                    if dset_index == 1:\n",
    "                        continue\n",
    "                    for trial_index in range(len(datasets[dset_index])):\n",
    "  \n",
    "                        tmp = datasets[dset_index][trial_index]['trace_seg']\n",
    "                        increment = tmp.shape[0]\n",
    "                        if not increment:\n",
    "                            continue\n",
    "\n",
    "                        train_trace[count:count+increment,:] = tmp\n",
    "                        train_rate[count:count+increment,:] = datasets[dset_index][trial_index]['rate_seg']\n",
    "                        train_spike[count:count+increment,:] = datasets[dset_index][trial_index]['spike_seg']\n",
    "                        count += increment\n",
    "\n",
    "            train_trace = train_trace[0:count,:]\n",
    "            train_rate = train_rate[0:count,:]\n",
    "            train_spike = train_spike[0:count,:]\n",
    "\n",
    "            if hour != 'all':\n",
    "                np.random.seed(dsets)\n",
    "                shrink_idx = (np.random.random_sample(np.ceil(float(hour)*3600/(opt.signal_len/opt.sampling_rate)).astype('int')) * count).astype('int')\n",
    "                train_trace = train_trace[shrink_idx,:]\n",
    "                train_rate = train_rate[shrink_idx,:]\n",
    "                train_spike = train_spike[shrink_idx,:]\n",
    "            \n",
    "            if np.sum(train_trace==np.nan) or np.sum(train_rate==np.nan) or np.sum(train_spike==np.nan):\n",
    "                print('NaN error...')\n",
    "                break\n",
    "\n",
    "            Training_dataset = TensorDataset(torch.FloatTensor(train_trace),torch.FloatTensor(train_rate),torch.FloatTensor(train_spike))\n",
    "            Training_dataloader = DataLoader(Training_dataset, shuffle=True, batch_size=opt.batch_size)\n",
    "\n",
    "            tqdm.write(f'dataset {dsets}, pair sample {len(Training_dataset)}: start training...')\n",
    "            \n",
    "            #### initiate network\n",
    "            \n",
    "            if nets=='UNet':\n",
    "                if kernel == '3':\n",
    "                    kernel_size, padding_size = 3, 1\n",
    "#                     print('Using UNet with 3-size kernels')\n",
    "                elif kernel == '5':\n",
    "                    kernel_size, padding_size = 5, 2\n",
    "                    print('Using UNet with 5-size kernels')\n",
    "                elif kernel == '7':\n",
    "                    kernel_size, padding_size = 7, 3\n",
    "                    print('Using UNet with 7-size kernels')\n",
    "                else:\n",
    "                    print('Network error...')\n",
    "                    break\n",
    "\n",
    "                if node == '50K':\n",
    "                    init_features_num = 5\n",
    "                    print('Using UNet with 50K nodes')\n",
    "                elif node == '150K':\n",
    "                    init_features_num = 9\n",
    "#                     print('Using UNet with 150K nodes')\n",
    "                elif node == '450K':\n",
    "                    init_features_num = 16\n",
    "                    print('Using UNet with 450K nodes')\n",
    "                else:\n",
    "                    print('Network error...')\n",
    "                    break\n",
    "            \n",
    "                C = UNet(init_features=init_features_num, kernel_size=kernel_size, padding=padding_size).cuda()\n",
    "            elif nets=='LeNet':\n",
    "                C = LeNet().cuda()\n",
    "            elif nets=='FCNet':\n",
    "                C = FCNet().cuda()\n",
    "            else:\n",
    "                print('Network error...')\n",
    "                break\n",
    "                \n",
    "            C.apply(weights_init_normal)\n",
    "            C_optimizer = optim.Adam(C.parameters(), lr=opt.lr)\n",
    "#             print('Learning rate is '+str(opt.lr))\n",
    "\n",
    "            mse_loss = torch.nn.MSELoss().cuda()\n",
    "\n",
    "            early_stopping = EarlyStopping(patience=opt.patience, verbose=True, delta=0.0000)\n",
    "            is_earlystop = 0\n",
    "            \n",
    "            #### start training\n",
    "            start_time = datetime.datetime.now()\n",
    "            t = trange(1, opt.epochs+1, leave=True,  ncols=1000)\n",
    "            for epoch in t:\n",
    "\n",
    "                # extract training batch\n",
    "                np.random.seed(epoch)\n",
    "                rand_idx = (np.random.random_sample(opt.batch_size) * len(Training_dataset)).astype('int')\n",
    "                [train_trace, train_rate, train_spike] = Training_dataset[rand_idx]\n",
    "                train_trace = Variable(train_trace.type(FloatTensor))\n",
    "                train_rate = Variable(train_rate.type(FloatTensor))\n",
    "                train_spike = Variable(train_spike.type(FloatTensor))\n",
    "                \n",
    "                # train model\n",
    "                C_optimizer.zero_grad()\n",
    "                C.train()\n",
    "\n",
    "                if losses=='MSE':\n",
    "                    loss = mse_loss(C(train_trace), train_rate)\n",
    "                elif losses=='EucD':\n",
    "                    loss = eucd_loss(C(train_trace), train_rate, train_spike)\n",
    "                elif losses=='Corr':\n",
    "                    loss = -pearson_corr_loss(C(train_trace), train_rate)\n",
    "                else:\n",
    "                    print('Loss error...')\n",
    "                    break\n",
    "\n",
    "                loss.backward()\n",
    "                C_optimizer.step()\n",
    "\n",
    "                early_stopping(loss.item(), C)\n",
    "                if early_stopping.early_stop:\n",
    "                    print('Early stopping...Epoch '+str(epoch))\n",
    "                    is_earlystop = 1\n",
    "                    \n",
    "                # gather status\n",
    "                t.set_description(inputs+' '+nets+' ['+losses+': %0.3f]' % loss.item())\n",
    "\n",
    "                self.DATA[dsets-1]['loss'].append(loss.item())\n",
    "                \n",
    "                # check performance\n",
    "                if (epoch % (opt.sample_interval) == 0) or is_earlystop:\n",
    "\n",
    "                    elapsed_time = datetime.datetime.now() - start_time\n",
    "                    tqdm.write(f'[{inputs}][{nets}][{losses}]【Dset {dsets}】[Ep {epoch}][Time {str(elapsed_time):.7s}]')\n",
    "                \n",
    "                    for sub_i, (sub_trace,sub_rate,sub_spike,sub_event) in enumerate(zip(test_trace,test_rate,test_spike,test_event)):\n",
    "                    \n",
    "                        Testing_dataset = TensorDataset(torch.FloatTensor(sub_trace),torch.FloatTensor(sub_rate),torch.FloatTensor(sub_spike))\n",
    "                        Testing_dataloader = DataLoader(Testing_dataset, shuffle=False, batch_size=opt.batch_size*4)\n",
    "                        \n",
    "                        # testing\n",
    "                        start_time = datetime.datetime.now()\n",
    "                        C.eval()\n",
    "                        with torch.no_grad():\n",
    "                            calcium = torch.zeros((0,opt.signal_len))\n",
    "                            pd_rate_tmp = torch.zeros((0,opt.signal_len)).type(FloatTensor).cuda()\n",
    "                            gt_rate = torch.zeros((0,opt.signal_len))\n",
    "                            gt_spike = torch.zeros((0,opt.signal_len))\n",
    "\n",
    "                            for batch, (test_data, test_rate_seg, test_spike_seg) in enumerate(Testing_dataloader):\n",
    "                                test_output = C(Variable(test_data.type(FloatTensor)))\n",
    "\n",
    "                                calcium = torch.cat([calcium, test_data], axis=0)\n",
    "                                pd_rate_tmp = torch.cat([pd_rate_tmp, test_output], axis=0)\n",
    "                                gt_rate = torch.cat([gt_rate, test_rate_seg], axis=0)\n",
    "                                gt_spike = torch.cat([gt_spike, test_spike_seg], axis=0)\n",
    "\n",
    "                        calcium = calcium[:, [opt.signal_len//2]].cpu().numpy()\n",
    "                        gt_rate = gt_rate[:,[opt.signal_len//2]].cpu().numpy()\n",
    "                        gt_spike = gt_spike[:,[opt.signal_len//2]].cpu().numpy()\n",
    "                        pd_rate_tmp = pd_rate_tmp.cpu().numpy()\n",
    "\n",
    "                        pd_rate = np.zeros((1,gt_rate.shape[0]+opt.signal_len-1))\n",
    "                        for align_idx in range(pd_rate_tmp.shape[0]):\n",
    "                            pd_rate[:,align_idx:align_idx+opt.signal_len] += pd_rate_tmp[align_idx,:]\n",
    "                        pd_rate = pd_rate[:,opt.signal_len//2:opt.signal_len//2+gt_rate.shape[0]].transpose()/opt.signal_len\n",
    "\n",
    "                        if (epoch == opt.epochs) or is_earlystop:\n",
    "                            if verbose:\n",
    "                                print('Estimate spike...')\n",
    "                            pd_spike = estimate_spike(pd_rate, std=opt.smoothing)\n",
    "                            pd_event = extract_event(pd_spike)\n",
    "                            gt_event = extract_event(gt_spike)\n",
    "                        else:\n",
    "                            pd_spike = pd_rate * 0\n",
    "\n",
    "                        elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                        self.DATA[dsets-1]['calcium'].append(calcium.squeeze())\n",
    "                        self.DATA[dsets-1]['gt_rate'].append(gt_rate.squeeze())\n",
    "                        self.DATA[dsets-1]['gt_spike'].append(gt_spike.squeeze())\n",
    "                        self.DATA[dsets-1]['pd_rate'].append(pd_rate.squeeze())\n",
    "                        self.DATA[dsets-1]['pd_spike'].append(pd_spike.squeeze())\n",
    "                        self.DATA[dsets-1]['gt_event'].append(gt_event)\n",
    "                        self.DATA[dsets-1]['pd_event'].append(pd_event)\n",
    "                        self.DATA[dsets-1]['events_times'].append(sub_event)\n",
    "            \n",
    "                        # compute Corr, Error, Bias\n",
    "                        test_corr, test_error, test_bias = evaluate_cascade(pd_rate, gt_rate)\n",
    "\n",
    "                        self.DATA[dsets-1]['correlation'].append(test_corr)\n",
    "                        self.DATA[dsets-1]['error'].append(test_error)\n",
    "                        self.DATA[dsets-1]['bias'].append(test_bias)\n",
    "\n",
    "                        # compute van Rossum distance\n",
    "                        test_eucd = evaluate_eucd(pd_rate, gt_rate, gt_spike)\n",
    "\n",
    "                        self.DATA[dsets-1]['eucd'].append(test_eucd)\n",
    "\n",
    "                        # compute Victor-Purpura distance\n",
    "                        if verbose:\n",
    "                            print('Compute vpd...')\n",
    "                        test_vpd = evaluate_vpd(pd_event, sub_event)\n",
    "\n",
    "                        self.DATA[dsets-1]['vpd'].append(test_vpd)\n",
    "\n",
    "                        # compute Error Rate\n",
    "                        if verbose:\n",
    "                            print('Compute error rate...')\n",
    "                        test_gter50 = evaluate_er(extract_event(estimate_spike(gt_rate, std=opt.smoothing)), sub_event)\n",
    "                        test_er50 = evaluate_er(pd_event, sub_event, 0.05) # ER with  50ms window\n",
    "\n",
    "                        self.DATA[dsets-1]['gter50'].append(test_gter50)\n",
    "                        self.DATA[dsets-1]['er50'].append(test_er50)\n",
    "\n",
    "                        tqdm.write(f'[Dset {dsets}][Neuron {sub_i+1}]【Corr {test_corr*100:.2f}%】[vRD {test_eucd:.2f}][VPd {test_vpd:.2f}]【ER50: {test_er50*100:.2f}%】[GTER50: {test_gter50*100:.2f}%][Error {test_error:.2f}][Bias {test_bias:.2f}][Time {str(elapsed_time):.7s}]')\n",
    "                \n",
    "                if is_earlystop:\n",
    "                    break\n",
    "\n",
    "            self.MODEL[dsets] = C\n",
    "\n",
    "            del Training_dataset, Training_dataloader, Testing_dataset, Testing_dataloader\n",
    "            del train_trace, train_rate, train_spike\n",
    "            del C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【C220817_Exc_60Hz_Raw_UNet_MSE_Test】\n",
      "dataset 2: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Re-compiling datasets...\n",
      "[D 1]  Raw Fs:  11.5  Noise:  0.7  FR:  1.8  #Neuron:  21  #Trial:  21  #Sample:  522354  Duration:  145  Name: DS01-OGB1-m-V1\n",
      "[D 2]  Raw Fs:  15.1  Noise:  0.5  FR:  0.2  #Neuron:  16  #Trial:  47  #Sample:  415612  Duration:  115  Name: DS02-OGB1-2-m-V1\n",
      "[D 3]  Raw Fs: 500.0  Noise:  0.3  FR:  1.2  #Neuron:   8  #Trial: 134  #Sample:   60217  Duration:   16  Name: DS03-Cal520-m-S1\n",
      "[D 4]  Raw Fs:   7.7  Noise:  1.0  FR:  0.4  #Neuron:  15  #Trial:  42  #Sample:  269902  Duration:   74  Name: DS04-OGB1-zf-pDp\n",
      "[D 5]  Raw Fs:   7.8  Noise:  1.6  FR:  1.3  #Neuron:   5  #Trial:  16  #Sample:  112243  Duration:   31  Name: DS05-Cal520-zf-pDp\n",
      "[D 6]  Raw Fs:  30.0  Noise:  1.3  FR:  1.9  #Neuron:   8  #Trial:  23  #Sample:  163944  Duration:   45  Name: DS06-GCaMP6f-zf-aDp\n",
      "[D 7]  Raw Fs:  30.0  Noise:  0.6  FR:  1.5  #Neuron:  10  #Trial:  35  #Sample:  247082  Duration:   68  Name: DS07-GCaMP6f-zf-dD\n",
      "[D 8]  Raw Fs:  30.0  Noise:  0.8  FR:  5.3  #Neuron:   9  #Trial:  23  #Sample:  161554  Duration:   44  Name: DS08-GCaMP6f-zf-OB\n",
      "[D 9]  Raw Fs:  60.1  Noise:  0.4  FR:  0.6  #Neuron:  11  #Trial:  33  #Sample:  462967  Duration:  128  Name: DS09-GCaMP6f-m-V1\n",
      "[D10]  Raw Fs: 160.1  Noise:  0.5  FR:  1.6  #Neuron:  23  #Trial:  23  #Sample:  256470  Duration:   71  Name: DS10-GCaMP6f-m-V1-ne\n",
      "[D11]  Raw Fs: 158.3  Noise:  0.5  FR:  1.5  #Neuron:  25  #Trial:  25  #Sample:  268776  Duration:   74  Name: DS11-GCaMP6f-m-V1-ne\n",
      "[D12]  Raw Fs: 151.6  Noise:  0.8  FR:  1.0  #Neuron:   6  #Trial:   6  #Sample:   47704  Duration:   13  Name: DS12-GCaMP6s-m-V1-ne\n",
      "[D13]  Raw Fs: 157.5  Noise:  0.5  FR:  1.3  #Neuron:  26  #Trial:  26  #Sample:  223145  Duration:   61  Name: DS13-GCaMP6s-m-V1-ne\n",
      "[D14]  Raw Fs:  60.1  Noise:  0.5  FR:  0.4  #Neuron:   7  #Trial:  18  #Sample:  250675  Duration:   69  Name: DS14-GCaMP6s-m-V1\n",
      "[D15]  Raw Fs:  59.1  Noise:  0.7  FR:  2.7  #Neuron:   9  #Trial:   9  #Sample:  277379  Duration:   77  Name: DS15-GCaMP6s-m-V1\n",
      "[D16]  Raw Fs:  59.1  Noise:  0.9  FR:  5.8  #Neuron:   9  #Trial:   9  #Sample:   90819  Duration:   25  Name: DS16-GCaMP6s-m-V1\n",
      "[D17]  Raw Fs:  50.0  Noise:  0.5  FR:  1.6  #Neuron:   9  #Trial:   9  #Sample:  103862  Duration:   28  Name: DS17-GCaMP5k-m-V1\n",
      "[D18]  Raw Fs:  20.0  Noise:  1.6  FR:  2.2  #Neuron:   4  #Trial:  67  #Sample:  115767  Duration:   32  Name: DS18-R-CaMP-m-CA3\n",
      "[D19]  Raw Fs:  14.4  Noise:  0.6  FR:  0.9  #Neuron:   9  #Trial:  96  #Sample:  166837  Duration:   46  Name: DS19-R-CaMP-m-S1\n",
      "[D20]  Raw Fs:  15.0  Noise:  1.2  FR:  0.6  #Neuron:   9  #Trial:  17  #Sample:  295165  Duration:   81  Name: DS20-jRCaMP1a-m-V1\n",
      "[D21]  Raw Fs:  29.8  Noise:  1.0  FR:  1.5  #Neuron:  11  #Trial:  25  #Sample:  413504  Duration:  114  Name: DS21-jGECO1a-m-V1\n",
      "[D22]  Raw Fs:  15.6  Noise:  0.6  FR:  1.1  #Neuron:   5  #Trial:  20  #Sample:  176820  Duration:   49  Name: DS22-OGB1-m-SST-V1\n",
      "[D23]  Raw Fs:  15.6  Noise:  0.6  FR:  6.9  #Neuron:   7  #Trial:   7  #Sample:   61887  Duration:   17  Name: DS23-OGB1-m-PV-V1\n",
      "[D24]  Raw Fs:  26.6  Noise:  0.5  FR: 11.6  #Neuron:  13  #Trial:  37  #Sample:  767701  Duration:  213  Name: DS24-GCaMP6f-m-PV-V1\n",
      "[D25]  Raw Fs:  26.6  Noise:  0.6  FR:  5.9  #Neuron:  17  #Trial:  80  #Sample: 1338336  Duration:  371  Name: DS25-GCaMP6f-m-SST-V\n",
      "[D26]  Raw Fs:  26.6  Noise:  2.1  FR:  5.5  #Neuron:  11  #Trial:  37  #Sample:  893894  Duration:  248  Name: DS26-GCaMP6f-m-VIP-V\n",
      "[D27]  Raw Fs:  30.0  Noise:  2.8  FR:  7.0  #Neuron:   4  #Trial:   4  #Sample:  106726  Duration:   29  Name: DS27-GCaMP6f-m-PV-vi\n",
      "dataset 2, pair sample 3988012: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71106018c68144b2b2aa580633ab423c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 2581\n",
      "[Raw][UNet][MSE]【Dset 2】[Ep 2581][Time 0:02:31]\n",
      "[Dset 2][Neuron 1]【Corr 65.23%】[vRD 2.71][VPd 0.88]【ER50: 78.63%】[GTER50: 0.43%][Error 0.91][Bias -0.76][Time 0:00:01]\n",
      "[Dset 2][Neuron 2]【Corr 72.17%】[vRD 2.66][VPd 0.76]【ER50: 61.46%】[GTER50: 1.60%][Error 0.81][Bias -0.63][Time 0:00:01]\n",
      "[Dset 2][Neuron 3]【Corr 51.71%】[vRD 3.08][VPd 0.93]【ER50: 93.33%】[GTER50: 1.75%][Error 1.07][Bias -0.70][Time 0:00:00]\n",
      "[Dset 2][Neuron 4]【Corr 67.31%】[vRD 3.51][VPd 0.97]【ER50: 95.51%】[GTER50: 0.00%][Error 0.97][Bias -0.86][Time 0:00:00]\n",
      "[Dset 2][Neuron 5]【Corr 58.97%】[vRD 2.74][VPd 0.96]【ER50: 92.86%】[GTER50: 0.00%][Error 1.06][Bias -0.67][Time 0:00:01]\n",
      "[Dset 2][Neuron 6]【Corr 65.40%】[vRD 3.11][VPd 0.76]【ER50: 65.71%】[GTER50: 0.88%][Error 1.00][Bias -0.59][Time 0:00:00]\n",
      "[Dset 2][Neuron 7]【Corr 52.17%】[vRD 2.73][VPd 0.90]【ER50: 87.50%】[GTER50: 0.00%][Error 1.20][Bias -0.49][Time 0:00:00]\n",
      "[Dset 2][Neuron 8]【Corr 53.69%】[vRD 2.61][VPd 0.83]【ER50: 70.00%】[GTER50: 0.00%][Error 1.12][Bias -0.53][Time 0:00:00]\n",
      "[Dset 2][Neuron 9]【Corr 53.77%】[vRD 3.02][VPd 0.89]【ER50: 80.00%】[GTER50: 0.00%][Error 1.25][Bias -0.45][Time 0:00:01]\n",
      "[Dset 2][Neuron 10]【Corr 53.19%】[vRD 2.87][VPd 0.95]【ER50: 90.91%】[GTER50: 2.33%][Error 1.00][Bias -0.87][Time 0:00:00]\n",
      "[Dset 2][Neuron 11]【Corr 56.92%】[vRD 2.75][VPd 0.94]【ER50: 88.24%】[GTER50: 0.00%][Error 1.05][Bias -0.76][Time 0:00:02]\n",
      "[Dset 2][Neuron 12]【Corr 27.20%】[vRD 2.80][VPd 0.98]【ER50: 97.16%】[GTER50: 0.18%][Error 1.01][Bias -0.92][Time 0:00:01]\n",
      "[Dset 2][Neuron 13]【Corr 64.74%】[vRD 2.99][VPd 0.90]【ER50: 81.40%】[GTER50: 0.00%][Error 0.97][Bias -0.74][Time 0:00:01]\n",
      "[Dset 2][Neuron 14]【Corr 64.43%】[vRD 2.74][VPd 0.83]【ER50: 74.01%】[GTER50: 0.00%][Error 0.91][Bias -0.70][Time 0:00:00]\n",
      "[Dset 2][Neuron 15]【Corr 61.08%】[vRD 2.78][VPd 0.86]【ER50: 78.02%】[GTER50: 0.21%][Error 0.91][Bias -0.80][Time 0:00:00]\n",
      "[Dset 2][Neuron 16]【Corr 36.21%】[vRD 2.77][VPd 0.99]【ER50: 94.34%】[GTER50: 0.00%][Error 1.01][Bias -0.81][Time 0:00:00]\n",
      "dataset 3: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 3, pair sample 4343407: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dd392c5a1044d7a3c826dc7c4cc894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 2656\n",
      "[Raw][UNet][MSE]【Dset 3】[Ep 2656][Time 0:02:34]\n",
      "[Dset 3][Neuron 1]【Corr 31.88%】[vRD 2.89][VPd 0.57]【ER50: 75.00%】[GTER50: 1.05%][Error 1.16][Bias -0.28][Time 0:00:00]\n",
      "[Dset 3][Neuron 2]【Corr 38.39%】[vRD 3.29][VPd 0.53]【ER50: 75.61%】[GTER50: 2.33%][Error 1.91][Bias 0.55][Time 0:00:00]\n",
      "[Dset 3][Neuron 3]【Corr 45.87%】[vRD 2.77][VPd 0.47]【ER50: 58.40%】[GTER50: 0.62%][Error 1.43][Bias 0.04][Time 0:00:00]\n",
      "[Dset 3][Neuron 4]【Corr 27.65%】[vRD 3.20][VPd 0.34]【ER50: 80.52%】[GTER50: 1.28%][Error 1.57][Bias 0.21][Time 0:00:00]\n",
      "[Dset 3][Neuron 5]【Corr 41.94%】[vRD 3.64][VPd 0.27]【ER50: 58.38%】[GTER50: 0.81%][Error 1.44][Bias 0.21][Time 0:00:00]\n",
      "[Dset 3][Neuron 6]【Corr 35.28%】[vRD 3.54][VPd 0.36]【ER50: 63.11%】[GTER50: 0.56%][Error 1.24][Bias -0.10][Time 0:00:00]\n",
      "[Dset 3][Neuron 7]【Corr 32.73%】[vRD 3.90][VPd 0.33]【ER50: 61.81%】[GTER50: 1.41%][Error 1.32][Bias -0.06][Time 0:00:00]\n",
      "[Dset 3][Neuron 8]【Corr 39.91%】[vRD 3.97][VPd 0.29]【ER50: 65.35%】[GTER50: 2.33%][Error 1.65][Bias 0.37][Time 0:00:00]\n",
      "dataset 4: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 4, pair sample 4133722: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021a797f89b7465380d75578f5872c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1579\n",
      "[Raw][UNet][MSE]【Dset 4】[Ep 1579][Time 0:01:32]\n",
      "[Dset 4][Neuron 1]【Corr 58.53%】[vRD 2.67][VPd 0.48]【ER50: 46.67%】[GTER50: 0.00%][Error 1.67][Bias 0.81][Time 0:00:00]\n",
      "[Dset 4][Neuron 2]【Corr 58.99%】[vRD 2.55][VPd 0.21]【ER50: 33.33%】[GTER50: 0.00%][Error 3.80][Bias 3.04][Time 0:00:00]\n",
      "[Dset 4][Neuron 3]【Corr 66.16%】[vRD 2.53][VPd 0.24]【ER50: 31.82%】[GTER50: 0.00%][Error 2.57][Bias 1.94][Time 0:00:00]\n",
      "[Dset 4][Neuron 4]【Corr 62.33%】[vRD 2.29][VPd 0.40]【ER50: 39.78%】[GTER50: 0.00%][Error 2.00][Bias 1.04][Time 0:00:00]\n",
      "[Dset 4][Neuron 5]【Corr 51.83%】[vRD 2.36][VPd 0.52]【ER50: 56.06%】[GTER50: 0.13%][Error 1.31][Bias 0.07][Time 0:00:01]\n",
      "[Dset 4][Neuron 6]【Corr 44.85%】[vRD 2.53][VPd 0.55]【ER50: 57.14%】[GTER50: 0.00%][Error 1.51][Bias 0.28][Time 0:00:00]\n",
      "[Dset 4][Neuron 7]【Corr 50.93%】[vRD 2.60][VPd 0.67]【ER50: 64.09%】[GTER50: 1.54%][Error 1.15][Bias -0.23][Time 0:00:01]\n",
      "[Dset 4][Neuron 8]【Corr 40.00%】[vRD 4.60][VPd 4.06]【ER50: 83.33%】[GTER50: 0.00%][Error 8.60][Bias 7.99][Time 0:00:00]\n",
      "[Dset 4][Neuron 9]【Corr 64.08%】[vRD 2.97][VPd 0.83]【ER50: 50.00%】[GTER50: 0.00%][Error 2.84][Bias 2.27][Time 0:00:00]\n",
      "[Dset 4][Neuron 10]【Corr 41.09%】[vRD 2.46][VPd 0.45]【ER50: 61.90%】[GTER50: 0.00%][Error 1.53][Bias 0.19][Time 0:00:00]\n",
      "[Dset 4][Neuron 11]【Corr 52.42%】[vRD 2.40][VPd 0.58]【ER50: 58.14%】[GTER50: 0.53%][Error 1.01][Bias -0.34][Time 0:00:01]\n",
      "[Dset 4][Neuron 12]【Corr 58.73%】[vRD 3.38][VPd 1.42]【ER50: 41.18%】[GTER50: 0.00%][Error 9.10][Bias 8.71][Time 0:00:00]\n",
      "[Dset 4][Neuron 13]【Corr 58.80%】[vRD 2.44][VPd 0.44]【ER50: 48.79%】[GTER50: 0.40%][Error 1.40][Bias 0.19][Time 0:00:00]\n",
      "[Dset 4][Neuron 14]【Corr 64.92%】[vRD 3.34][VPd 1.02]【ER50: 33.33%】[GTER50: 0.00%][Error 11.87][Bias 11.66][Time 0:00:00]\n",
      "[Dset 4][Neuron 15]【Corr 50.60%】[vRD 2.84][VPd 0.75]【ER50: 57.89%】[GTER50: 0.00%][Error 2.66][Bias 1.45][Time 0:00:00]\n",
      "dataset 5: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 5, pair sample 4291381: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5312e2a23284316be36c9c43e1186f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1709\n",
      "[Raw][UNet][MSE]【Dset 5】[Ep 1709][Time 0:01:39]\n",
      "[Dset 5][Neuron 1]【Corr 54.10%】[vRD 4.50][VPd 1.64]【ER50: 56.13%】[GTER50: 0.00%][Error 2.45][Bias 2.04][Time 0:00:00]\n",
      "[Dset 5][Neuron 2]【Corr 40.60%】[vRD 5.49][VPd 3.50]【ER50: 77.29%】[GTER50: 0.00%][Error 4.66][Bias 4.06][Time 0:00:00]\n",
      "[Dset 5][Neuron 3]【Corr 63.20%】[vRD 4.03][VPd 1.60]【ER50: 53.67%】[GTER50: 0.34%][Error 2.45][Bias 2.06][Time 0:00:00]\n",
      "[Dset 5][Neuron 4]【Corr 58.85%】[vRD 2.90][VPd 0.58]【ER50: 47.15%】[GTER50: 0.13%][Error 1.48][Bias 0.77][Time 0:00:01]\n",
      "[Dset 5][Neuron 5]【Corr 74.00%】[vRD 10.93][VPd 6.13]【ER50: 75.38%】[GTER50: 0.00%][Error 10.27][Bias 10.27][Time 0:00:00]\n",
      "dataset 6: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 6, pair sample 4239680: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c8513cf96a43a5a296dd3bcc5d517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1492\n",
      "[Raw][UNet][MSE]【Dset 6】[Ep 1492][Time 0:01:27]\n",
      "[Dset 6][Neuron 1]【Corr 62.87%】[vRD 2.17][VPd 0.37]【ER50: 33.93%】[GTER50: 0.71%][Error 0.98][Bias 0.30][Time 0:00:01]\n",
      "[Dset 6][Neuron 2]【Corr 55.71%】[vRD 3.63][VPd 1.14]【ER50: 47.16%】[GTER50: 0.47%][Error 1.78][Bias 1.35][Time 0:00:00]\n",
      "[Dset 6][Neuron 3]【Corr 57.77%】[vRD 4.00][VPd 1.65]【ER50: 54.13%】[GTER50: 0.66%][Error 2.40][Bias 2.00][Time 0:00:00]\n",
      "[Dset 6][Neuron 4]【Corr 70.90%】[vRD 2.58][VPd 0.73]【ER50: 35.92%】[GTER50: 1.44%][Error 1.40][Bias 0.93][Time 0:00:00]\n",
      "[Dset 6][Neuron 5]【Corr 73.91%】[vRD 2.06][VPd 0.40]【ER50: 28.68%】[GTER50: 1.22%][Error 1.00][Bias 0.38][Time 0:00:01]\n",
      "[Dset 6][Neuron 6]【Corr 66.78%】[vRD 2.17][VPd 0.40]【ER50: 30.07%】[GTER50: 0.00%][Error 1.32][Bias 0.71][Time 0:00:00]\n",
      "[Dset 6][Neuron 7]【Corr 78.68%】[vRD 1.79][VPd 0.23]【ER50: 20.38%】[GTER50: 1.06%][Error 0.79][Bias 0.16][Time 0:00:00]\n",
      "[Dset 6][Neuron 8]【Corr 79.58%】[vRD 1.68][VPd 0.25]【ER50: 19.54%】[GTER50: 0.74%][Error 0.65][Bias -0.03][Time 0:00:01]\n",
      "dataset 7: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 7, pair sample 4156542: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b8b42fb0b94efca8243c4ec3437908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 889\n",
      "[Raw][UNet][MSE]【Dset 7】[Ep 889][Time 0:00:52]\n",
      "[Dset 7][Neuron 1]【Corr 59.35%】[vRD 3.11][VPd 0.59]【ER50: 49.68%】[GTER50: 1.00%][Error 1.30][Bias 0.33][Time 0:00:00]\n",
      "[Dset 7][Neuron 2]【Corr 61.46%】[vRD 2.29][VPd 0.36]【ER50: 39.42%】[GTER50: 0.33%][Error 1.08][Bias 0.29][Time 0:00:01]\n",
      "[Dset 7][Neuron 3]【Corr 64.51%】[vRD 2.17][VPd 0.45]【ER50: 26.96%】[GTER50: 1.10%][Error 1.21][Bias 0.59][Time 0:00:00]\n",
      "[Dset 7][Neuron 4]【Corr 62.59%】[vRD 2.24][VPd 0.39]【ER50: 34.45%】[GTER50: 0.00%][Error 1.11][Bias 0.41][Time 0:00:01]\n",
      "[Dset 7][Neuron 5]【Corr 66.71%】[vRD 2.10][VPd 0.29]【ER50: 31.62%】[GTER50: 0.17%][Error 1.01][Bias 0.24][Time 0:00:00]\n",
      "[Dset 7][Neuron 6]【Corr 52.34%】[vRD 2.24][VPd 0.45]【ER50: 47.31%】[GTER50: 0.12%][Error 0.94][Bias -0.19][Time 0:00:00]\n",
      "[Dset 7][Neuron 7]【Corr 68.04%】[vRD 2.16][VPd 0.34]【ER50: 23.18%】[GTER50: 0.08%][Error 1.12][Bias 0.48][Time 0:00:01]\n",
      "[Dset 7][Neuron 8]【Corr 77.37%】[vRD 2.01][VPd 0.30]【ER50: 20.22%】[GTER50: 0.20%][Error 1.10][Bias 0.53][Time 0:00:00]\n",
      "[Dset 7][Neuron 9]【Corr 57.11%】[vRD 2.36][VPd 0.46]【ER50: 40.24%】[GTER50: 0.94%][Error 1.02][Bias 0.05][Time 0:00:00]\n",
      "[Dset 7][Neuron 10]【Corr 74.87%】[vRD 1.97][VPd 0.26]【ER50: 19.78%】[GTER50: 0.20%][Error 0.94][Bias 0.33][Time 0:00:01]\n",
      "dataset 8: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 8, pair sample 4242070: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c04f6914464efb81f727931cb60eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 702\n",
      "[Raw][UNet][MSE]【Dset 8】[Ep 702][Time 0:00:41]\n",
      "[Dset 8][Neuron 1]【Corr 72.08%】[vRD 3.89][VPd 0.68]【ER50: 52.48%】[GTER50: 1.42%][Error 0.74][Bias -0.63][Time 0:00:00]\n",
      "[Dset 8][Neuron 2]【Corr 67.84%】[vRD 2.65][VPd 0.79]【ER50: 65.64%】[GTER50: 2.57%][Error 0.78][Bias -0.71][Time 0:00:00]\n",
      "[Dset 8][Neuron 3]【Corr 65.53%】[vRD 3.07][VPd 0.81]【ER50: 68.58%】[GTER50: 3.48%][Error 0.80][Bias -0.75][Time 0:00:00]\n",
      "[Dset 8][Neuron 4]【Corr 78.69%】[vRD 2.31][VPd 0.59]【ER50: 45.57%】[GTER50: 2.90%][Error 0.67][Bias -0.50][Time 0:00:01]\n",
      "[Dset 8][Neuron 5]【Corr 67.62%】[vRD 2.15][VPd 0.29]【ER50: 31.61%】[GTER50: 1.51%][Error 0.71][Bias -0.14][Time 0:00:00]\n",
      "[Dset 8][Neuron 6]【Corr 84.46%】[vRD 1.83][VPd 0.42]【ER50: 30.13%】[GTER50: 2.24%][Error 0.56][Bias -0.32][Time 0:00:01]\n",
      "[Dset 8][Neuron 7]【Corr 60.18%】[vRD 2.60][VPd 0.82]【ER50: 71.32%】[GTER50: 2.65%][Error 0.82][Bias -0.74][Time 0:00:00]\n",
      "[Dset 8][Neuron 8]【Corr 68.50%】[vRD 1.95][VPd 0.36]【ER50: 30.55%】[GTER50: 0.30%][Error 0.75][Bias -0.20][Time 0:00:00]\n",
      "[Dset 8][Neuron 9]【Corr 67.76%】[vRD 3.02][VPd 0.53]【ER50: 48.17%】[GTER50: 1.76%][Error 0.82][Bias -0.40][Time 0:00:01]\n",
      "dataset 9: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 9, pair sample 3940657: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d40452ce07046b893a9f35601b75ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1249\n",
      "[Raw][UNet][MSE]【Dset 9】[Ep 1249][Time 0:01:12]\n",
      "[Dset 9][Neuron 1]【Corr 71.84%】[vRD 2.85][VPd 0.45]【ER50: 28.92%】[GTER50: 0.89%][Error 1.09][Bias 0.57][Time 0:00:01]\n",
      "[Dset 9][Neuron 2]【Corr 75.51%】[vRD 2.11][VPd 0.40]【ER50: 32.01%】[GTER50: 0.56%][Error 0.84][Bias -0.10][Time 0:00:00]\n",
      "[Dset 9][Neuron 3]【Corr 69.74%】[vRD 3.74][VPd 0.80]【ER50: 35.88%】[GTER50: 0.34%][Error 1.33][Bias 0.92][Time 0:00:00]\n",
      "[Dset 9][Neuron 4]【Corr 82.81%】[vRD 1.81][VPd 0.49]【ER50: 32.45%】[GTER50: 1.39%][Error 0.64][Bias -0.23][Time 0:00:02]\n",
      "[Dset 9][Neuron 5]【Corr 79.93%】[vRD 2.61][VPd 0.52]【ER50: 23.46%】[GTER50: 0.00%][Error 1.38][Bias 1.00][Time 0:00:01]\n",
      "[Dset 9][Neuron 6]【Corr 71.95%】[vRD 2.00][VPd 0.28]【ER50: 20.78%】[GTER50: 0.17%][Error 1.08][Bias 0.34][Time 0:00:01]\n",
      "[Dset 9][Neuron 7]【Corr 86.64%】[vRD 1.65][VPd 0.31]【ER50: 22.04%】[GTER50: 0.75%][Error 0.77][Bias 0.11][Time 0:00:01]\n",
      "[Dset 9][Neuron 8]【Corr 74.60%】[vRD 2.20][VPd 0.28]【ER50: 26.61%】[GTER50: 0.93%][Error 0.80][Bias -0.06][Time 0:00:02]\n",
      "[Dset 9][Neuron 9]【Corr 80.59%】[vRD 1.93][VPd 0.49]【ER50: 33.01%】[GTER50: 0.37%][Error 0.82][Bias -0.19][Time 0:00:01]\n",
      "[Dset 9][Neuron 10]【Corr 70.69%】[vRD 2.45][VPd 0.46]【ER50: 39.49%】[GTER50: 1.15%][Error 0.97][Bias -0.09][Time 0:00:02]\n",
      "[Dset 9][Neuron 11]【Corr 75.18%】[vRD 2.16][VPd 0.36]【ER50: 26.48%】[GTER50: 1.15%][Error 0.85][Bias -0.06][Time 0:00:01]\n",
      "dataset 10: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 10, pair sample 4147154: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88854952a124a9bb67f1b5bf796f03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1583\n",
      "[Raw][UNet][MSE]【Dset 10】[Ep 1583][Time 0:01:32]\n",
      "[Dset 10][Neuron 1]【Corr 92.76%】[vRD 2.30][VPd 0.48]【ER50: 23.13%】[GTER50: 1.08%][Error 0.74][Bias 0.53][Time 0:00:00]\n",
      "[Dset 10][Neuron 2]【Corr 93.11%】[vRD 1.62][VPd 0.23]【ER50: 13.07%】[GTER50: 1.27%][Error 0.50][Bias 0.27][Time 0:00:00]\n",
      "[Dset 10][Neuron 3]【Corr 87.07%】[vRD 1.84][VPd 0.32]【ER50: 23.08%】[GTER50: 0.49%][Error 0.75][Bias 0.09][Time 0:00:00]\n",
      "[Dset 10][Neuron 4]【Corr 89.74%】[vRD 1.97][VPd 0.37]【ER50: 22.00%】[GTER50: 1.10%][Error 1.16][Bias 0.74][Time 0:00:00]\n",
      "[Dset 10][Neuron 5]【Corr 93.00%】[vRD 1.63][VPd 0.27]【ER50: 14.65%】[GTER50: 0.59%][Error 0.48][Bias 0.03][Time 0:00:00]\n",
      "[Dset 10][Neuron 6]【Corr 89.15%】[vRD 1.86][VPd 0.34]【ER50: 23.54%】[GTER50: 0.44%][Error 0.50][Bias -0.16][Time 0:00:00]\n",
      "[Dset 10][Neuron 7]【Corr 88.29%】[vRD 1.98][VPd 0.31]【ER50: 20.44%】[GTER50: 1.34%][Error 0.54][Bias -0.04][Time 0:00:00]\n",
      "[Dset 10][Neuron 8]【Corr 89.74%】[vRD 1.58][VPd 0.27]【ER50: 17.76%】[GTER50: 0.50%][Error 0.41][Bias -0.06][Time 0:00:00]\n",
      "[Dset 10][Neuron 9]【Corr 92.48%】[vRD 1.78][VPd 0.25]【ER50: 17.52%】[GTER50: 0.44%][Error 0.38][Bias -0.13][Time 0:00:00]\n",
      "[Dset 10][Neuron 10]【Corr 89.69%】[vRD 3.05][VPd 0.59]【ER50: 43.34%】[GTER50: 0.66%][Error 0.66][Bias -0.51][Time 0:00:00]\n",
      "[Dset 10][Neuron 11]【Corr 63.50%】[vRD 3.89][VPd 0.64]【ER50: 44.39%】[GTER50: 1.52%][Error 0.91][Bias -0.19][Time 0:00:00]\n",
      "[Dset 10][Neuron 12]【Corr 93.73%】[vRD 2.62][VPd 0.53]【ER50: 35.05%】[GTER50: 0.39%][Error 0.66][Bias -0.35][Time 0:00:00]\n",
      "[Dset 10][Neuron 13]【Corr 90.17%】[vRD 1.88][VPd 0.39]【ER50: 23.44%】[GTER50: 0.74%][Error 0.63][Bias 0.06][Time 0:00:00]\n",
      "[Dset 10][Neuron 14]【Corr 78.85%】[vRD 2.70][VPd 0.63]【ER50: 47.37%】[GTER50: 0.68%][Error 1.11][Bias -0.01][Time 0:00:00]\n",
      "[Dset 10][Neuron 15]【Corr 88.36%】[vRD 1.91][VPd 0.22]【ER50: 18.59%】[GTER50: 0.89%][Error 0.43][Bias -0.11][Time 0:00:00]\n",
      "[Dset 10][Neuron 16]【Corr 80.49%】[vRD 2.66][VPd 0.28]【ER50: 25.64%】[GTER50: 1.32%][Error 0.57][Bias -0.09][Time 0:00:00]\n",
      "[Dset 10][Neuron 17]【Corr 92.75%】[vRD 1.65][VPd 0.30]【ER50: 18.79%】[GTER50: 0.89%][Error 0.44][Bias -0.13][Time 0:00:00]\n",
      "[Dset 10][Neuron 18]【Corr 85.86%】[vRD 2.53][VPd 0.52]【ER50: 31.86%】[GTER50: 0.59%][Error 0.88][Bias 0.36][Time 0:00:00]\n",
      "[Dset 10][Neuron 19]【Corr 92.92%】[vRD 1.56][VPd 0.27]【ER50: 16.27%】[GTER50: 0.87%][Error 0.50][Bias 0.10][Time 0:00:00]\n",
      "[Dset 10][Neuron 20]【Corr 90.61%】[vRD 2.15][VPd 0.26]【ER50: 16.55%】[GTER50: 0.23%][Error 0.47][Bias 0.16][Time 0:00:00]\n",
      "[Dset 10][Neuron 21]【Corr 89.67%】[vRD 2.15][VPd 0.30]【ER50: 20.70%】[GTER50: 0.47%][Error 0.50][Bias -0.01][Time 0:00:00]\n",
      "[Dset 10][Neuron 22]【Corr 89.83%】[vRD 1.92][VPd 0.42]【ER50: 30.82%】[GTER50: 0.53%][Error 0.82][Bias 0.02][Time 0:00:00]\n",
      "[Dset 10][Neuron 23]【Corr 87.82%】[vRD 1.88][VPd 0.47]【ER50: 29.67%】[GTER50: 0.00%][Error 1.03][Bias 0.32][Time 0:00:00]\n",
      "dataset 11: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 11, pair sample 4134848: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7be6cbd93444b0694c91b1dcdd34957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1150\n",
      "[Raw][UNet][MSE]【Dset 11】[Ep 1150][Time 0:01:07]\n",
      "[Dset 11][Neuron 1]【Corr 78.97%】[vRD 2.79][VPd 0.63]【ER50: 52.50%】[GTER50: 0.65%][Error 0.97][Bias -0.35][Time 0:00:00]\n",
      "[Dset 11][Neuron 2]【Corr 83.47%】[vRD 2.77][VPd 0.55]【ER50: 42.64%】[GTER50: 0.62%][Error 1.19][Bias 0.11][Time 0:00:00]\n",
      "[Dset 11][Neuron 3]【Corr 74.79%】[vRD 2.62][VPd 0.70]【ER50: 51.43%】[GTER50: 0.00%][Error 1.18][Bias -0.14][Time 0:00:00]\n",
      "[Dset 11][Neuron 4]【Corr 47.85%】[vRD 3.37][VPd 1.22]【ER50: 61.76%】[GTER50: 0.00%][Error 2.03][Bias 0.95][Time 0:00:00]\n",
      "[Dset 11][Neuron 5]【Corr 88.47%】[vRD 1.94][VPd 0.32]【ER50: 21.86%】[GTER50: 1.89%][Error 0.68][Bias 0.16][Time 0:00:00]\n",
      "[Dset 11][Neuron 6]【Corr 85.09%】[vRD 1.93][VPd 0.39]【ER50: 22.54%】[GTER50: 0.00%][Error 0.83][Bias 0.32][Time 0:00:00]\n",
      "[Dset 11][Neuron 7]【Corr 34.97%】[vRD 2.84][VPd 0.96]【ER50: 70.43%】[GTER50: 0.00%][Error 2.22][Bias 0.77][Time 0:00:00]\n",
      "[Dset 11][Neuron 8]【Corr 31.86%】[vRD 2.68][VPd 0.91]【ER50: 87.10%】[GTER50: 0.00%][Error 1.08][Bias -0.73][Time 0:00:00]\n",
      "[Dset 11][Neuron 9]【Corr 82.01%】[vRD 1.72][VPd 0.35]【ER50: 24.61%】[GTER50: 1.45%][Error 0.65][Bias -0.18][Time 0:00:00]\n",
      "[Dset 11][Neuron 10]【Corr 80.19%】[vRD 2.05][VPd 0.50]【ER50: 36.27%】[GTER50: 1.39%][Error 0.90][Bias -0.11][Time 0:00:00]\n",
      "[Dset 11][Neuron 11]【Corr 87.19%】[vRD 1.86][VPd 0.29]【ER50: 18.63%】[GTER50: 0.46%][Error 0.80][Bias 0.10][Time 0:00:00]\n",
      "[Dset 11][Neuron 12]【Corr 76.26%】[vRD 2.47][VPd 0.69]【ER50: 54.46%】[GTER50: 0.58%][Error 0.83][Bias -0.49][Time 0:00:00]\n",
      "[Dset 11][Neuron 13]【Corr 63.43%】[vRD 2.74][VPd 0.79]【ER50: 58.23%】[GTER50: 2.76%][Error 1.38][Bias 0.17][Time 0:00:00]\n",
      "[Dset 11][Neuron 14]【Corr 84.04%】[vRD 2.16][VPd 0.48]【ER50: 34.96%】[GTER50: 1.18%][Error 0.68][Bias -0.32][Time 0:00:00]\n",
      "[Dset 11][Neuron 15]【Corr 72.09%】[vRD 3.14][VPd 0.72]【ER50: 63.77%】[GTER50: 0.00%][Error 1.21][Bias -0.23][Time 0:00:00]\n",
      "[Dset 11][Neuron 16]【Corr 76.48%】[vRD 2.86][VPd 0.63]【ER50: 43.17%】[GTER50: 0.00%][Error 1.10][Bias -0.06][Time 0:00:00]\n",
      "[Dset 11][Neuron 17]【Corr 84.85%】[vRD 2.20][VPd 0.55]【ER50: 38.97%】[GTER50: 1.12%][Error 0.66][Bias -0.42][Time 0:00:00]\n",
      "[Dset 11][Neuron 18]【Corr 82.36%】[vRD 2.53][VPd 0.59]【ER50: 43.65%】[GTER50: 1.17%][Error 0.68][Bias -0.49][Time 0:00:00]\n",
      "[Dset 11][Neuron 19]【Corr 69.07%】[vRD 2.72][VPd 0.80]【ER50: 67.90%】[GTER50: 0.78%][Error 1.23][Bias -0.30][Time 0:00:00]\n",
      "[Dset 11][Neuron 20]【Corr 81.63%】[vRD 2.16][VPd 0.56]【ER50: 40.03%】[GTER50: 1.38%][Error 0.64][Bias -0.46][Time 0:00:00]\n",
      "[Dset 11][Neuron 21]【Corr 88.86%】[vRD 3.53][VPd 0.60]【ER50: 44.22%】[GTER50: 0.81%][Error 0.67][Bias -0.56][Time 0:00:00]\n",
      "[Dset 11][Neuron 22]【Corr 86.25%】[vRD 2.08][VPd 0.49]【ER50: 34.32%】[GTER50: 1.30%][Error 0.57][Bias -0.42][Time 0:00:00]\n",
      "[Dset 11][Neuron 23]【Corr 77.24%】[vRD 2.60][VPd 0.69]【ER50: 55.03%】[GTER50: 1.46%][Error 0.76][Bias -0.60][Time 0:00:00]\n",
      "[Dset 11][Neuron 24]【Corr 77.88%】[vRD 2.54][VPd 0.61]【ER50: 46.39%】[GTER50: 1.26%][Error 0.74][Bias -0.50][Time 0:00:00]\n",
      "[Dset 11][Neuron 25]【Corr 51.00%】[vRD 2.85][VPd 0.87]【ER50: 80.00%】[GTER50: 1.64%][Error 1.27][Bias -0.45][Time 0:00:00]\n",
      "dataset 12: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 12, pair sample 4355920: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5777247e36b54212a1bd37ce6c9ad644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 2317\n",
      "[Raw][UNet][MSE]【Dset 12】[Ep 2317][Time 0:02:15]\n",
      "[Dset 12][Neuron 1]【Corr 90.25%】[vRD 2.31][VPd 0.53]【ER50: 40.05%】[GTER50: 0.77%][Error 0.73][Bias -0.31][Time 0:00:00]\n",
      "[Dset 12][Neuron 2]【Corr 77.45%】[vRD 3.59][VPd 1.49]【ER50: 48.19%】[GTER50: 0.00%][Error 2.63][Bias 2.30][Time 0:00:00]\n",
      "[Dset 12][Neuron 3]【Corr 87.69%】[vRD 1.71][VPd 0.25]【ER50: 14.39%】[GTER50: 0.80%][Error 0.86][Bias 0.48][Time 0:00:00]\n",
      "[Dset 12][Neuron 4]【Corr 93.43%】[vRD 1.53][VPd 0.29]【ER50: 12.39%】[GTER50: 0.00%][Error 0.76][Bias 0.66][Time 0:00:00]\n",
      "[Dset 12][Neuron 5]【Corr 90.60%】[vRD 1.48][VPd 0.21]【ER50: 11.56%】[GTER50: 1.04%][Error 0.77][Bias 0.37][Time 0:00:00]\n",
      "[Dset 12][Neuron 6]【Corr 79.97%】[vRD 2.03][VPd 0.43]【ER50: 24.43%】[GTER50: 0.00%][Error 1.23][Bias 0.63][Time 0:00:00]\n",
      "dataset 13: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 13, pair sample 4180479: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f1fb3b845847099990f5ca5528fc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1856\n",
      "[Raw][UNet][MSE]【Dset 13】[Ep 1856][Time 0:01:48]\n",
      "[Dset 13][Neuron 1]【Corr 80.63%】[vRD 2.17][VPd 0.50]【ER50: 27.17%】[GTER50: 0.44%][Error 1.08][Bias 0.70][Time 0:00:00]\n",
      "[Dset 13][Neuron 2]【Corr 86.95%】[vRD 2.83][VPd 0.80]【ER50: 30.88%】[GTER50: 0.00%][Error 1.41][Bias 1.27][Time 0:00:00]\n",
      "[Dset 13][Neuron 3]【Corr 87.04%】[vRD 1.94][VPd 0.45]【ER50: 26.87%】[GTER50: 1.05%][Error 0.99][Bias 0.55][Time 0:00:00]\n",
      "[Dset 13][Neuron 4]【Corr 74.03%】[vRD 2.80][VPd 0.51]【ER50: 38.10%】[GTER50: 0.43%][Error 0.87][Bias -0.02][Time 0:00:00]\n",
      "[Dset 13][Neuron 5]【Corr 87.91%】[vRD 2.07][VPd 0.33]【ER50: 25.08%】[GTER50: 0.62%][Error 0.74][Bias 0.07][Time 0:00:00]\n",
      "[Dset 13][Neuron 6]【Corr 88.84%】[vRD 2.40][VPd 0.34]【ER50: 28.05%】[GTER50: 0.61%][Error 0.88][Bias 0.23][Time 0:00:00]\n",
      "[Dset 13][Neuron 7]【Corr 88.60%】[vRD 2.14][VPd 0.43]【ER50: 28.50%】[GTER50: 0.47%][Error 0.82][Bias 0.13][Time 0:00:00]\n",
      "[Dset 13][Neuron 8]【Corr 94.73%】[vRD 3.44][VPd 1.09]【ER50: 35.20%】[GTER50: 0.95%][Error 1.47][Bias 1.45][Time 0:00:00]\n",
      "[Dset 13][Neuron 9]【Corr 81.83%】[vRD 2.80][VPd 0.83]【ER50: 43.36%】[GTER50: 0.00%][Error 2.27][Bias 1.65][Time 0:00:00]\n",
      "[Dset 13][Neuron 10]【Corr 79.35%】[vRD 2.36][VPd 0.40]【ER50: 31.06%】[GTER50: 0.65%][Error 0.73][Bias -0.05][Time 0:00:00]\n",
      "[Dset 13][Neuron 11]【Corr 82.29%】[vRD 2.38][VPd 0.36]【ER50: 25.65%】[GTER50: 0.24%][Error 0.87][Bias 0.27][Time 0:00:00]\n",
      "[Dset 13][Neuron 12]【Corr 79.10%】[vRD 2.20][VPd 0.42]【ER50: 32.51%】[GTER50: 0.31%][Error 0.96][Bias 0.22][Time 0:00:00]\n",
      "[Dset 13][Neuron 13]【Corr 82.65%】[vRD 2.22][VPd 0.38]【ER50: 29.08%】[GTER50: 0.40%][Error 0.93][Bias 0.24][Time 0:00:00]\n",
      "[Dset 13][Neuron 14]【Corr 85.37%】[vRD 2.32][VPd 0.33]【ER50: 26.54%】[GTER50: 0.23%][Error 0.77][Bias 0.17][Time 0:00:00]\n",
      "[Dset 13][Neuron 15]【Corr 83.09%】[vRD 2.07][VPd 0.33]【ER50: 24.73%】[GTER50: 0.24%][Error 0.72][Bias -0.06][Time 0:00:00]\n",
      "[Dset 13][Neuron 16]【Corr 76.05%】[vRD 2.20][VPd 0.32]【ER50: 24.84%】[GTER50: 0.82%][Error 0.80][Bias 0.07][Time 0:00:00]\n",
      "[Dset 13][Neuron 17]【Corr 86.17%】[vRD 2.52][VPd 0.80]【ER50: 33.73%】[GTER50: 1.19%][Error 1.31][Bias 0.95][Time 0:00:00]\n",
      "[Dset 13][Neuron 18]【Corr 78.52%】[vRD 3.01][VPd 1.52]【ER50: 47.83%】[GTER50: 2.40%][Error 2.39][Bias 2.02][Time 0:00:00]\n",
      "[Dset 13][Neuron 19]【Corr 85.79%】[vRD 1.67][VPd 0.26]【ER50: 14.91%】[GTER50: 0.63%][Error 0.72][Bias 0.19][Time 0:00:00]\n",
      "[Dset 13][Neuron 20]【Corr 72.63%】[vRD 2.23][VPd 0.63]【ER50: 37.57%】[GTER50: 1.78%][Error 1.54][Bias 0.76][Time 0:00:00]\n",
      "[Dset 13][Neuron 21]【Corr 81.95%】[vRD 1.79][VPd 0.30]【ER50: 21.79%】[GTER50: 0.62%][Error 0.83][Bias 0.30][Time 0:00:00]\n",
      "[Dset 13][Neuron 22]【Corr 81.83%】[vRD 1.89][VPd 0.28]【ER50: 23.79%】[GTER50: 1.12%][Error 0.72][Bias 0.12][Time 0:00:00]\n",
      "[Dset 13][Neuron 23]【Corr 84.96%】[vRD 1.82][VPd 0.30]【ER50: 20.32%】[GTER50: 0.89%][Error 0.81][Bias 0.44][Time 0:00:00]\n",
      "[Dset 13][Neuron 24]【Corr 75.94%】[vRD 2.60][VPd 0.93]【ER50: 35.95%】[GTER50: 1.62%][Error 1.43][Bias 1.09][Time 0:00:00]\n",
      "[Dset 13][Neuron 25]【Corr 73.33%】[vRD 2.09][VPd 0.39]【ER50: 31.54%】[GTER50: 0.74%][Error 1.15][Bias 0.30][Time 0:00:00]\n",
      "[Dset 13][Neuron 26]【Corr 63.31%】[vRD 2.45][VPd 0.80]【ER50: 48.19%】[GTER50: 0.00%][Error 2.01][Bias 1.14][Time 0:00:00]\n",
      "dataset 14: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 14, pair sample 4152949: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4262e4b8b1de4a71b8bcef43b7ec9aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1573\n",
      "[Raw][UNet][MSE]【Dset 14】[Ep 1573][Time 0:01:31]\n",
      "[Dset 14][Neuron 1]【Corr 65.61%】[vRD 4.48][VPd 3.40]【ER50: 62.86%】[GTER50: 0.00%][Error 5.39][Bias 5.26][Time 0:00:00]\n",
      "[Dset 14][Neuron 2]【Corr 79.66%】[vRD 2.09][VPd 0.24]【ER50: 23.48%】[GTER50: 0.11%][Error 1.22][Bias 0.55][Time 0:00:01]\n",
      "[Dset 14][Neuron 3]【Corr 77.88%】[vRD 2.78][VPd 1.29]【ER50: 41.05%】[GTER50: 0.00%][Error 4.41][Bias 4.07][Time 0:00:00]\n",
      "[Dset 14][Neuron 4]【Corr 77.53%】[vRD 3.29][VPd 1.26]【ER50: 39.92%】[GTER50: 1.00%][Error 2.08][Bias 1.94][Time 0:00:01]\n",
      "[Dset 14][Neuron 5]【Corr 83.82%】[vRD 1.77][VPd 0.15]【ER50: 15.16%】[GTER50: 0.33%][Error 0.98][Bias 0.39][Time 0:00:01]\n",
      "[Dset 14][Neuron 6]【Corr 67.18%】[vRD 5.15][VPd 4.06]【ER50: 66.93%】[GTER50: 1.18%][Error 8.29][Bias 8.28][Time 0:00:01]\n",
      "[Dset 14][Neuron 7]【Corr 74.85%】[vRD 2.70][VPd 0.83]【ER50: 33.21%】[GTER50: 1.04%][Error 1.38][Bias 1.07][Time 0:00:01]\n",
      "dataset 15: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 15, pair sample 4126245: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a05bd05a8f447a8d2d774fd651af4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1605\n",
      "[Raw][UNet][MSE]【Dset 15】[Ep 1605][Time 0:01:33]\n",
      "[Dset 15][Neuron 1]【Corr 61.40%】[vRD 2.84][VPd 0.41]【ER50: 45.82%】[GTER50: 1.95%][Error 0.88][Bias -0.23][Time 0:00:01]\n",
      "[Dset 15][Neuron 2]【Corr 55.12%】[vRD 2.75][VPd 0.36]【ER50: 48.02%】[GTER50: 1.26%][Error 0.99][Bias -0.06][Time 0:00:01]\n",
      "[Dset 15][Neuron 3]【Corr 45.91%】[vRD 3.05][VPd 0.51]【ER50: 65.63%】[GTER50: 1.05%][Error 1.19][Bias -0.15][Time 0:00:01]\n",
      "[Dset 15][Neuron 4]【Corr 57.60%】[vRD 3.20][VPd 0.47]【ER50: 48.20%】[GTER50: 1.67%][Error 1.02][Bias -0.08][Time 0:00:01]\n",
      "[Dset 15][Neuron 5]【Corr 55.86%】[vRD 2.99][VPd 0.57]【ER50: 55.16%】[GTER50: 1.99%][Error 1.05][Bias -0.18][Time 0:00:00]\n",
      "[Dset 15][Neuron 6]【Corr 61.72%】[vRD 2.79][VPd 0.44]【ER50: 50.39%】[GTER50: 1.95%][Error 0.91][Bias -0.24][Time 0:00:00]\n",
      "[Dset 15][Neuron 7]【Corr 55.25%】[vRD 3.31][VPd 0.49]【ER50: 54.14%】[GTER50: 1.36%][Error 0.98][Bias -0.28][Time 0:00:02]\n",
      "[Dset 15][Neuron 8]【Corr 52.56%】[vRD 3.11][VPd 0.47]【ER50: 56.75%】[GTER50: 0.83%][Error 1.04][Bias -0.22][Time 0:00:01]\n",
      "[Dset 15][Neuron 9]【Corr 63.56%】[vRD 2.90][VPd 0.50]【ER50: 47.81%】[GTER50: 1.58%][Error 0.92][Bias -0.20][Time 0:00:01]\n",
      "dataset 16: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 16, pair sample 4312805: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0018b80dddc4de2ae24430bc635ec01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 808\n",
      "[Raw][UNet][MSE]【Dset 16】[Ep 808][Time 0:00:47]\n",
      "[Dset 16][Neuron 1]【Corr 56.87%】[vRD 2.40][VPd 0.61]【ER50: 57.47%】[GTER50: 1.05%][Error 0.88][Bias -0.44][Time 0:00:00]\n",
      "[Dset 16][Neuron 2]【Corr 60.05%】[vRD 2.23][VPd 0.53]【ER50: 49.29%】[GTER50: 0.84%][Error 0.89][Bias -0.32][Time 0:00:00]\n",
      "[Dset 16][Neuron 3]【Corr 61.39%】[vRD 3.58][VPd 0.85]【ER50: 76.55%】[GTER50: 1.33%][Error 0.88][Bias -0.77][Time 0:00:00]\n",
      "[Dset 16][Neuron 4]【Corr 66.32%】[vRD 3.24][VPd 0.87]【ER50: 77.56%】[GTER50: 1.89%][Error 0.84][Bias -0.81][Time 0:00:00]\n",
      "[Dset 16][Neuron 5]【Corr 69.68%】[vRD 4.35][VPd 0.83]【ER50: 74.61%】[GTER50: 0.69%][Error 0.95][Bias -0.72][Time 0:00:00]\n",
      "[Dset 16][Neuron 6]【Corr 58.07%】[vRD 3.76][VPd 0.81]【ER50: 75.00%】[GTER50: 0.84%][Error 0.92][Bias -0.70][Time 0:00:00]\n",
      "[Dset 16][Neuron 7]【Corr 67.84%】[vRD 3.85][VPd 0.81]【ER50: 71.05%】[GTER50: 1.20%][Error 0.88][Bias -0.74][Time 0:00:00]\n",
      "[Dset 16][Neuron 8]【Corr 58.42%】[vRD 4.54][VPd 0.88]【ER50: 81.91%】[GTER50: 1.28%][Error 0.92][Bias -0.84][Time 0:00:00]\n",
      "[Dset 16][Neuron 9]【Corr 30.11%】[vRD 3.90][VPd 0.85]【ER50: 79.23%】[GTER50: 1.79%][Error 0.90][Bias -0.80][Time 0:00:00]\n",
      "dataset 17: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 17, pair sample 4299762: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b24760326194cb5b84f631ae7b7aedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1093\n",
      "[Raw][UNet][MSE]【Dset 17】[Ep 1093][Time 0:01:03]\n",
      "[Dset 17][Neuron 1]【Corr 72.48%】[vRD 2.24][VPd 0.56]【ER50: 43.03%】[GTER50: 0.30%][Error 1.23][Bias 0.31][Time 0:00:00]\n",
      "[Dset 17][Neuron 2]【Corr 83.36%】[vRD 2.19][VPd 0.51]【ER50: 38.07%】[GTER50: 2.29%][Error 0.78][Bias -0.18][Time 0:00:00]\n",
      "[Dset 17][Neuron 3]【Corr 89.62%】[vRD 2.30][VPd 0.54]【ER50: 37.26%】[GTER50: 1.52%][Error 0.61][Bias -0.45][Time 0:00:00]\n",
      "[Dset 17][Neuron 4]【Corr 69.35%】[vRD 2.09][VPd 0.43]【ER50: 37.79%】[GTER50: 0.98%][Error 0.89][Bias -0.12][Time 0:00:00]\n",
      "[Dset 17][Neuron 5]【Corr 84.39%】[vRD 2.00][VPd 0.59]【ER50: 35.38%】[GTER50: 0.00%][Error 1.32][Bias 0.75][Time 0:00:00]\n",
      "[Dset 17][Neuron 6]【Corr 81.63%】[vRD 1.82][VPd 0.46]【ER50: 35.17%】[GTER50: 0.53%][Error 0.84][Bias -0.07][Time 0:00:00]\n",
      "[Dset 17][Neuron 7]【Corr 80.34%】[vRD 2.63][VPd 0.59]【ER50: 46.49%】[GTER50: 1.80%][Error 0.84][Bias -0.32][Time 0:00:00]\n",
      "[Dset 17][Neuron 8]【Corr 83.38%】[vRD 2.18][VPd 0.50]【ER50: 38.55%】[GTER50: 2.33%][Error 0.72][Bias -0.34][Time 0:00:00]\n",
      "[Dset 17][Neuron 9]【Corr 74.79%】[vRD 2.46][VPd 0.49]【ER50: 43.12%】[GTER50: 1.57%][Error 0.77][Bias -0.33][Time 0:00:00]\n",
      "dataset 18: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 18, pair sample 4287857: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7a05bc69f4414881bbc4847914b735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 896\n",
      "[Raw][UNet][MSE]【Dset 18】[Ep 896][Time 0:00:52]\n",
      "[Dset 18][Neuron 1]【Corr 83.93%】[vRD 4.42][VPd 0.47]【ER50: 40.63%】[GTER50: 0.56%][Error 0.81][Bias -0.32][Time 0:00:02]\n",
      "[Dset 18][Neuron 2]【Corr 77.50%】[vRD 3.53][VPd 0.24]【ER50: 34.10%】[GTER50: 0.85%][Error 0.98][Bias 0.10][Time 0:00:01]\n",
      "[Dset 18][Neuron 3]【Corr 85.77%】[vRD 3.74][VPd 0.64]【ER50: 53.55%】[GTER50: 0.33%][Error 0.86][Bias -0.47][Time 0:00:00]\n",
      "[Dset 18][Neuron 4]【Corr 56.45%】[vRD 4.42][VPd 0.63]【ER50: 53.48%】[GTER50: 0.31%][Error 1.46][Bias 0.41][Time 0:00:00]\n",
      "dataset 19: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 19, pair sample 4236787: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565e9fd2a5874b61899a8021722902f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1684\n",
      "[Raw][UNet][MSE]【Dset 19】[Ep 1684][Time 0:01:38]\n",
      "[Dset 19][Neuron 1]【Corr 70.13%】[vRD 2.77][VPd 0.80]【ER50: 70.31%】[GTER50: 0.47%][Error 0.87][Bias -0.64][Time 0:00:00]\n",
      "[Dset 19][Neuron 2]【Corr 59.38%】[vRD 2.43][VPd 0.73]【ER50: 61.40%】[GTER50: 0.83%][Error 0.79][Bias -0.54][Time 0:00:00]\n",
      "[Dset 19][Neuron 3]【Corr 76.66%】[vRD 2.05][VPd 0.48]【ER50: 30.72%】[GTER50: 0.60%][Error 1.07][Bias 0.17][Time 0:00:01]\n",
      "[Dset 19][Neuron 4]【Corr 78.21%】[vRD 2.02][VPd 0.31]【ER50: 26.21%】[GTER50: 0.52%][Error 0.79][Bias 0.02][Time 0:00:01]\n",
      "[Dset 19][Neuron 5]【Corr 83.07%】[vRD 1.84][VPd 0.32]【ER50: 18.18%】[GTER50: 0.00%][Error 1.16][Bias 0.31][Time 0:00:00]\n",
      "[Dset 19][Neuron 6]【Corr 72.23%】[vRD 2.49][VPd 0.68]【ER50: 56.91%】[GTER50: 0.73%][Error 0.86][Bias -0.50][Time 0:00:01]\n",
      "[Dset 19][Neuron 7]【Corr 66.32%】[vRD 2.48][VPd 0.62]【ER50: 53.49%】[GTER50: 1.82%][Error 1.08][Bias -0.12][Time 0:00:00]\n",
      "[Dset 19][Neuron 8]【Corr 71.07%】[vRD 2.69][VPd 0.73]【ER50: 61.54%】[GTER50: 1.40%][Error 0.92][Bias -0.53][Time 0:00:00]\n",
      "[Dset 19][Neuron 9]【Corr 51.64%】[vRD 3.42][VPd 1.45]【ER50: 54.55%】[GTER50: 0.00%][Error 3.29][Bias 2.53][Time 0:00:00]\n",
      "dataset 20: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 20, pair sample 4108459: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b281eabb976403aa057bb19d67655af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 797\n",
      "[Raw][UNet][MSE]【Dset 20】[Ep 797][Time 0:00:46]\n",
      "[Dset 20][Neuron 1]【Corr 62.89%】[vRD 4.24][VPd 0.66]【ER50: 63.74%】[GTER50: 1.09%][Error 1.15][Bias -0.33][Time 0:00:00]\n",
      "[Dset 20][Neuron 2]【Corr 73.39%】[vRD 3.50][VPd 0.61]【ER50: 57.02%】[GTER50: 0.66%][Error 1.34][Bias 0.06][Time 0:00:00]\n",
      "[Dset 20][Neuron 3]【Corr 43.55%】[vRD 6.37][VPd 3.43]【ER50: 70.37%】[GTER50: 0.00%][Error 21.90][Bias 21.29][Time 0:00:00]\n",
      "[Dset 20][Neuron 4]【Corr 77.36%】[vRD 2.87][VPd 0.61]【ER50: 58.54%】[GTER50: 0.00%][Error 0.98][Bias -0.41][Time 0:00:00]\n",
      "[Dset 20][Neuron 5]【Corr 73.35%】[vRD 2.35][VPd 0.53]【ER50: 43.48%】[GTER50: 0.00%][Error 1.50][Bias 0.43][Time 0:00:01]\n",
      "[Dset 20][Neuron 6]【Corr 64.87%】[vRD 2.74][VPd 0.57]【ER50: 49.67%】[GTER50: 0.14%][Error 1.15][Bias -0.02][Time 0:00:01]\n",
      "[Dset 20][Neuron 7]【Corr 76.96%】[vRD 3.43][VPd 0.50]【ER50: 46.34%】[GTER50: 0.00%][Error 1.53][Bias 0.40][Time 0:00:01]\n",
      "[Dset 20][Neuron 8]【Corr 40.25%】[vRD 4.04][VPd 0.82]【ER50: 71.99%】[GTER50: 0.44%][Error 1.18][Bias -0.41][Time 0:00:01]\n",
      "[Dset 20][Neuron 9]【Corr 70.50%】[vRD 3.71][VPd 0.45]【ER50: 49.06%】[GTER50: 0.17%][Error 1.08][Bias -0.11][Time 0:00:01]\n",
      "dataset 21: preparing data...\n",
      "Sampling rate is: 60.0Hz, smoothing window is: 25.0ms\n",
      "Using previous datasets...\n",
      "dataset 21, pair sample 3990120: start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b73a1ebc3646289c6187443c91dd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...Epoch 1716\n",
      "[Raw][UNet][MSE]【Dset 21】[Ep 1716][Time 0:01:39]\n",
      "[Dset 21][Neuron 1]【Corr 80.37%】[vRD 4.36][VPd 0.66]【ER50: 49.74%】[GTER50: 0.42%][Error 1.12][Bias -0.07][Time 0:00:01]\n",
      "[Dset 21][Neuron 2]【Corr 74.41%】[vRD 3.60][VPd 1.33]【ER50: 55.18%】[GTER50: 0.00%][Error 2.83][Bias 2.02][Time 0:00:02]\n",
      "[Dset 21][Neuron 3]【Corr 50.16%】[vRD 5.94][VPd 3.75]【ER50: 71.93%】[GTER50: 0.71%][Error 6.64][Bias 6.02][Time 0:00:01]\n",
      "[Dset 21][Neuron 4]【Corr 59.15%】[vRD 3.87][VPd 0.77]【ER50: 67.57%】[GTER50: 1.53%][Error 1.04][Bias -0.47][Time 0:00:00]\n",
      "[Dset 21][Neuron 5]【Corr 67.47%】[vRD 3.72][VPd 0.72]【ER50: 49.46%】[GTER50: 1.18%][Error 1.21][Bias 0.04][Time 0:00:01]\n",
      "[Dset 21][Neuron 6]【Corr 80.74%】[vRD 4.04][VPd 0.65]【ER50: 43.54%】[GTER50: 0.17%][Error 1.00][Bias -0.05][Time 0:00:03]\n",
      "[Dset 21][Neuron 7]【Corr 80.33%】[vRD 6.27][VPd 1.02]【ER50: 60.65%】[GTER50: 0.44%][Error 1.73][Bias 0.42][Time 0:00:01]\n",
      "[Dset 21][Neuron 8]【Corr 63.81%】[vRD 3.43][VPd 0.64]【ER50: 54.46%】[GTER50: 0.78%][Error 0.80][Bias -0.55][Time 0:00:00]\n",
      "[Dset 21][Neuron 9]【Corr 58.65%】[vRD 2.67][VPd 0.60]【ER50: 49.82%】[GTER50: 1.47%][Error 1.37][Bias 0.28][Time 0:00:00]\n",
      "[Dset 21][Neuron 10]【Corr 56.62%】[vRD 2.52][VPd 0.53]【ER50: 49.52%】[GTER50: 1.27%][Error 1.46][Bias 0.49][Time 0:00:00]\n",
      "[Dset 21][Neuron 11]【Corr 50.14%】[vRD 4.37][VPd 0.89]【ER50: 54.84%】[GTER50: 1.04%][Error 1.42][Bias 0.37][Time 0:00:01]\n",
      "############### END OF TESTING ###############\n"
     ]
    }
   ],
   "source": [
    "## Start benchmark\n",
    "\n",
    "benchmark = BENCHMARK()\n",
    "benchmark.train()\n",
    "\n",
    "# del benchmark\n",
    "print('############### END OF TESTING ###############')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Notes:</b>\n",
    "arguments could be changed to test other model configurations:\n",
    "\n",
    "    - neuron = 'Exc', 'Inh', 'Both'                # select excitatory or inhibitory neurons\n",
    "    - nets = 'UNet', 'LeNet', 'FCNet'              # define model structure\n",
    "    - losses = 'MSE', 'EucD', 'Corr'               # define loss function\n",
    "    - Fs = '60', '30', '15', '7.5'                 # define re-sampling rate\n",
    "    - smoothing_std = '0.025','0.05','0.1','0.2'   # define smoothing window size\n",
    "    - smoothing_kernel = 'gaussian', 'causal'      # define smoothing kernel shape\n",
    "    - cluster = 'None', 'anticluster', 'cluster'   # select indicator types (correspond to \"all\", \"different\", \"same\" in manuscript)\n",
    "    - hour = 'all', '20','10','5','1','0.5','0.1'  # amount of data used for training\n",
    "    - lr = '0.001', '0.005', '0.0002'              # define learning rate\n",
    "    - kernel = '3', '5', '7'                       # setting kernel sizes for U-Net\n",
    "    - node = '50K','150K','450K'                   # setting node numbers for U-Net\n",
    "    - seg = '32','64','96'                         # define segment length\n",
    "    - batch = '256','512','1024','2048'            # define batch size\n",
    "    - es = '2000','1000','500','300','100'         # define patience for early-stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For example: </b>\n",
    "<br>To test with different re-sampling rate, smoothing window size, and segment length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# benchmark = BENCHMARK()\n",
    "# benchmark.train(neuron='Exc', nets='UNet', losses='MSE', Fs='7.5', smoothing_std='0.2', seg='64')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
